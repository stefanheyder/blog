[
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "Stefan Heyder\nWiesenweg 21, 98693 Ilmenau\nstefan.heyder@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "The Gumbel distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nAsymptotics of estimators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nFisherian reduction for the t-Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nStefan Heyder\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html",
    "href": "posts/2024-07-07 asymptotics/index.html",
    "title": "Asymptotics of estimators",
    "section": "",
    "text": "In my PhD thesis, I compare two methods to perform optimal importance sampling: the Cross-Entropy method (CE) and Efficient Importance Sampling (EIS). There are several criteria that you may want to consider for this comparison, but without going into too much detail , in this post, I’ll focus on only one aspect: asymptotics, in particular asymptotic relative efficiencies.If you’re interested, check out the draft of my thesis on Github.\nIn the context of my thesis, this crops up as both methods are simulation-based. Usually, we have to resort to simulation techniques when analytical computation is too difficult, which is exactly the case here! Both methods actually want to solve an optimization problem in the background, solving for an optimal parameter \\(\\psi\\). Unfortunately, the optimization problems involve quantities that we have no (analytical) access to. We can, however, set up a simulation routine that provides an estimate \\(\\hat\\psi\\) of this optimal parameter.\nBut using simulations means that the output of both methods is now random, i.e. we can expect to get different results \\(\\hat\\psi\\) when we repeatedly apply the methods using different RNG seeds. This can be problematic: if there is too much variation the actual \\(\\hat\\psi\\) we obtain could be far away from the optimal \\(\\psi\\) and the performance of the methods suffers.\nThis is where asymptotics come into play. When statisticians talk about asymptotics for estimators, here \\(\\hat\\psi\\), they are usually concerned with two things: consistency and asymptotic normality. Consistency is a critical property of an estimator."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html#asymptotic-normality",
    "href": "posts/2024-07-07 asymptotics/index.html#asymptotic-normality",
    "title": "Asymptotics of estimators",
    "section": "Asymptotic normality",
    "text": "Asymptotic normality\n\n\n\n\n\n\nDefinition (central limit theorem)\n\n\n\nAgain, let \\((\\hat\\psi_{N})_{N \\in \\mathbf N}\\) be a sequence of estimator of \\(\\psi\\). If \\[\n\\sqrt {N} \\left( \\hat \\psi_N - \\psi \\right) \\stackrel{\\mathcal D}{\\to} \\mathcal N(0, \\Sigma),\n\\] we say that \\(\\hat\\psi_{N}\\) fulfills a central limit theorem with asymptotic covariance matrix \\(\\Sigma\\).\n\n\nIf we have a consistent estimator, notice that \\(\\hat\\psi_N - \\psi\\) goes to \\(0\\) as \\(N\\) goes to \\(\\infty\\) (in probability or almost surely, depending on the type of consistency). Multiplying by \\(\\sqrt{N}\\) “blows up” this error, essentially zooming in to what happens around the true value \\(\\psi\\). Note that any estimator that fulfills a central limit theorem is weakly consistent.\nWhy should we assume a normal distribution as the limiting distribution? At first this choice may seem surprising, maybe even restrictive. But it turns out that for large classes of estimators, e.g. M- and Z-estimators, we can proof such a central limit theorem.\nHow does having a central limit theorem help us? If we know \\(\\Sigma\\), we can use it get a heuristic on how large we should choose \\(N\\). Using the approximation \\[\n\\hat\\psi_{N} -\\psi \\approx \\mathcal N\\left(0, \\frac{\\Sigma}{N}\\right)\n\\] we can choose \\(N\\) such that, e.g., for a given error \\(\\varepsilon &gt; 0\\), the probability that \\(\\lVert \\hat \\psi_N - \\psi \\rVert &lt; \\varepsilon\\) becomes, say, at least \\(80\\%\\).\nAdditionally, if we have two estimators \\(\\hat \\psi^1\\) and \\(\\hat\\psi^{2}\\), both of which fulfill a central limit theorem with asymptotic covariance matrices \\(\\Sigma_1\\) and \\(\\Sigma_{2}\\), we can compare \\(\\Sigma_1\\) and \\(\\Sigma_2\\). The estimator with the “smaller” asymptotic covariance matrix is the one we should prefer.\nAs we are dealing with matrices it is not necessarily clear what smaller means, we could be interested in, e.g.\n\n\\(\\Sigma_1 \\succ \\Sigma_{2}\\), i.e. \\(\\Sigma_1 - \\Sigma_2\\) is postive definite, or\n\\(\\operatorname{trace} \\left( \\Sigma_{1} \\right) &gt; \\operatorname{trace} \\left( \\Sigma_{2} \\right)\\), i.e. the asymptotic mean squared error (MSE) is smaller for \\(\\psi_2\\).\n\nUsually the asymptotic covariance matrix \\(\\Sigma\\) is not known, usually because it depends on either the true parameter \\(\\psi\\). For a toy example, consider the normal distribution \\(\\mathcal N(\\mu, \\sigma^2)\\) where the mean \\(\\mu \\in \\mathbf R\\) and variance is \\(\\sigma^2 \\in \\mathbf R_{&gt; 0}\\). Then the sample mean \\(\\bar X_{N}\\) for samples \\(X_{1}, \\dots, X_{N} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal N(\\mu, \\sigma^{2})\\) as an estimator of \\(\\mu\\) fulfills a central limit theorem: \\[\n\\sqrt{N} (\\bar X_{N} - \\mu) \\stackrel{\\mathcal D}\\to \\mathcal N(0, \\sigma^{2}).\n\\] Here \\(\\sigma^2\\) may be unknown. In practice we can estimate it consistently by the sample variance \\(\\hat \\sigma_N^{2}\\), and so \\[\n\\frac{\\sqrt{N}}{\\sqrt{\\hat \\sigma_{N}^2}} \\left( \\bar X_{N} - \\mu \\right) \\stackrel{\\mathcal D}\\to \\mathcal N(0,1),\n\\] by Slutsky’s lemma. Thus, for large \\(N\\), we can the true asymptotic variance \\(\\sigma^2\\) by its consistent estimate \\(\\hat\\sigma_N^2\\), and proceed as above."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html#whats-next",
    "href": "posts/2024-07-07 asymptotics/index.html#whats-next",
    "title": "Asymptotics of estimators",
    "section": "What’s next?",
    "text": "What’s next?\nIn this post, I’ve tried to convey the usefulness of asymptotics, especially when it comes to comparing two estimators \\(\\psi\\). However, I have not given you any guidance on when an estimator is consistent and when it fulfills a central limit theorem. It turns out, that for many types of estimators we can obtain, under some assumptions, central limit theorems. These settings I will discuss in a following post."
  },
  {
    "objectID": "posts/2022-02-08 fisherian_reduction/index.html",
    "href": "posts/2022-02-08 fisherian_reduction/index.html",
    "title": "Fisherian reduction for the t-Test",
    "section": "",
    "text": "In the second chapter of (Cox 2006) the authors talks about a Fisherian reduction which I think of as a framework of doing inference given a sufficient statistic \\(S\\). An interesting point here is that one can use the conditional distribution of the data, \\(X_1, \\dots, X_n\\) say, on \\(S\\) to evaluate the fit of the model.\nIn this post I want to explore this concept in the setting of a standard \\(t\\)-Test, i.e. we have \\(X_i \\overset{\\text{i.i.d}}{\\sim} \\mathcal N(\\mu, \\sigma^2)\\). The parameter of interest is of course \\(\\mu\\) and \\(S = (\\bar X_n, \\hat\\sigma^2_{n})\\) is a sufficient statistic, with \\(\\hat\\sigma^2_n\\) the empirical variance.\nTo apply the Fisherian reduction we thus need to find the conditional distribution of \\(X = \\left(X_1, \\dots, X_n\\right)\\) on \\(\\bar X_n\\), i.e.\\(X | \\bar X_n\\). For this, let \\(A = \\left(A_1, B\\right)  \\in \\mathbf R^{n\\times n}\\) be an orthogonal matrix whose first column is \\[A_1 = \\left(\\frac 1 {\\sqrt{n}}, \\dots , \\frac 1 {\\sqrt{n}} \\right).\\]\nThen \\(Y = A^TX \\sim \\mathcal N \\left(\\mu A^T\\mu \\mathbf 1, \\sigma^2 I_{n}\\right)\\) and \\(Y = \\left(\\sqrt{n} \\bar X_n, Z\\right)\\) where \\(Z \\sim \\mathcal N \\left(\\sigma^2 I_{n - 1}\\right)\\), \\(Z\\) and \\(\\bar X_n\\) being independent.\nTransforming back we obtain the conditional distribution we sought: \\[ X | \\bar X_n \\sim AY | \\bar X_n \\mathbf 1 \\sim \\bar X_n + B Z\\]\nThe catch here is that \\(B\\) is a \\(n\\times (n - 1)\\) dimensional matrix, so \\(X | \\bar X_n\\) has a normal distribution of dimension \\(n-1\\), e.g. the variance-covariance matrix is rank-deficient.\nLet’s verify this through simulation.\n\n\nCode\nmu &lt;- 10\nsigma &lt;- 1\nn &lt;- 2\nm &lt;- 1000\nx &lt;- matrix(rnorm(n * m, mu, sigma), nrow= n)\n\ny &lt;- t(t(x) - colMeans(x))\n\ntransformed &lt;- c(matrix(c(1/sqrt(2), -1/sqrt(2)), nrow = 1) %*% y)\n\nplot(y[1,], y[2,])\n\nhist(transformed)\nprint(mean(transformed))\nprint(sd(transformed))\n\n\n\n\n\n\n\n\n\n[1] 0.007553103\n[1] 1.05018\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCox, D. R. 2006. Principles of Statistical Inference. Cambridge ; New York: Cambridge University Press."
  },
  {
    "objectID": "posts/2024-07-17 gumbel_distribution/index.html",
    "href": "posts/2024-07-17 gumbel_distribution/index.html",
    "title": "The Gumbel distribution",
    "section": "",
    "text": "I recently learned about the Gumbel softmax trick, which seemingly allows smooth sampling from a discrete distribution. In writing this post, I want to learn more about the Gumbel distribution that appears in this trick, details on the trick may follow in a separate post.\nThe main component in the softmax trick is the following distribution, due to Gumbel (aptly named after him).\n\n\n\n\n\n\nDefinition (Gumbel distribution)\n\n\n\nFor \\(\\mu \\in \\mathbf R\\) and \\(\\beta \\in \\mathbf R_{&gt; 0}\\) the \\(\\operatorname{Gumbel}(\\mu, \\beta)\\) distribution has the CDF \\[\n\\begin{align*}\n    F: (-\\infty, \\infty) \\to (0, 1) && x \\mapsto \\exp \\left( -\\exp \\left( - \\frac{x - \\mu}{\\beta} \\right)  \\right).\n\\end{align*}\n\\]\n\n\nLet us quickly ensure that this defines a distribution: this CDF is strictly monotonically increasing and for \\(x \\to -\\infty\\) it goes to \\(0\\), while for \\(x \\to \\infty\\) it goes to \\(\\exp(0) = 1\\). By differentiating the CDF we obtain the density (with respect to Lebesgue measure), which is \\[\n\\frac{1}{\\beta} \\exp \\left( - \\frac{x - \\mu}{\\beta} -\\exp \\left( - \\frac{x - \\mu}{\\beta} \\right) \\right),\n\\] so the distribution is a continuous one.\nSimulating from this distribution is straight-forward using the inverse CDF trick: simulate \\(U \\sim \\operatorname{Unif}(0,1)\\) and let \\[G = F^{-1}(U) = -\\beta\\log( -\\log (U))  + \\mu,\\] then \\(G \\sim \\operatorname{Gumbel}(\\mu, \\beta)\\). Let’s draw some samples from the standard Gumbel distribution, i.e. where \\(\\mu = 0\\) and \\(\\beta = 1\\).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sim_gumbel(n, mu, beta):\n    return - beta * np.log(-np.log(np.random.uniform(size=n))) + mu\n\ndef density_gumbel(x,mu,beta):\n    return (1/beta) * np.exp(-(x-mu)/beta-np.exp(-(x-mu)/beta))\n\nsims = sim_gumbel(1000, 0, 1)\nxmin, xmax = min(sims), max(sims)\nplt.hist(sims, bins=30, edgecolor='black', density=True)\nplt.plot(np.linspace(xmin, xmax, 1000), density_gumbel(np.linspace(xmin, xmax, 1000), 0, 1), 'r')\nplt.title(f\"{len(sims)} Gumbel(0,1) draws with mean {np.mean(sims):.2f} and standard deviation {np.std(sims):.2f}\")\nplt.axvline(np.mean(sims), color='grey', lw=2, linestyle=\"--\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom these simulations and the right skew visible in the density, we can guess that the mean of the standard Gumbel is not \\(0\\). Indeed, if \\(G \\sim \\operatorname{Gumbel}(0,1)\\), then 1 \\[\n    \\mathbf E G = \\int_{0}^1 F^{-1}(u) \\mathrm d u = \\int_{0}^1 -\\log( - \\log u) \\mathrm d u = \\gamma \\approx 0.5772,\n\\] where \\(\\gamma\\) is the Euler-Mascheroni constant.\n1 see wikipedia, I have not been able to find a concrete proof of this yetFrom the definition, we can see that the Gumbel distributions form a location-scale family, i.e. if \\(G \\sim  \\operatorname{Gumbel}(\\mu, \\beta)\\), then \\(aG + b \\sim \\operatorname{Gumbel}(a\\mu + b, a^2\\beta)\\). Similar to the normal distribution, this allows us to focus on the standard Gumbel distribution \\(\\operatorname{Gumbel}(0,1)\\).\nThe origins of the Gumbel distribution go back to the early 1930s, when Gumbel discovered the distribution as a limiting distribution of the maximum i.i.d. exponentially distributed samples. This makes the Gumbel distribution one of the three GEV (generalized extreme value) distributions, the others being the Fréchet and the reverse Weibull distribution. Actually, if \\(X \\sim \\operatorname{Gumbel}(0,1)\\), then \\(\\exp (X)\\) and \\(- \\exp(-X)\\) follow a Fréchet and a reverse Weibull distribution respectively.\n\n\n\n\n\n\nTheorem (limit theorem for the Gumbel distribution)\n\n\n\nFor \\(i \\in \\mathbf N\\) let \\(X_i \\stackrel{\\text{i.i.d.}}{\\sim} \\operatorname{Exp}(1)\\) and let \\(Y_n = \\max \\{X_1, \\dots, X_n\\}\\) be the maximum value in the first \\(n\\) samples. Then, as \\(n \\to \\infty\\), \\[\n    Y_{n} - \\log n \\stackrel{\\mathcal D}{\\longrightarrow} \\operatorname{Gumbel}(0, 1).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe CDF of \\(Y_n - \\log n\\) is, for \\(y \\geq - \\log n\\), \\[\n\\begin{align*}\n    \\mathbf P \\left( Y_{n} - \\log n \\leq y \\right) &=  \\mathbf P \\left( Y_{n} \\leq y + \\log n \\right) = (1 - \\exp(-(y + \\log n)))^{n} \\\\\n    &= \\left(1 - \\frac{\\exp(-y)}{n}\\right)^{n}\\to \\exp(-\\exp(-y)) = F(y).\n\\end{align*}\n\\]\n\n\n\nAgain, let’s verify by simulation that this is true, comparing to the density of the standard Gumbel distribution.\n\n\nCode\ndef sim_exponential(n, lambd):\n    return -1/lambd * np.log(np.random.uniform(size=n))\n\n\nfig, axs = plt.subplots(3,2)\nfig.tight_layout()\nns = [2, 4, 10, 20, 50, 100]\nm = 10000\n\nfor ax, n in zip(axs.flatten(), ns):\n    X = sim_exponential((m, n), 1)\n    Y = np.max(X, axis = 1)\n    Z = Y - np.log(n)\n    min_z, max_z = min(Z), max(Z)\n    ax.hist(Z, bins=100, density=True)\n    ax.plot(np.linspace(min_z, max_z, 1000), density_gumbel(np.linspace(min_z, max_z, 1000), 0, 1), 'r')\n    ax.set_title(f\"distribution of $Y_{{ {n} }} - \\\\log {n}$\")\n    # mode of gumbel distribution is at mu\n    ax.set_ylim(0, 1.2* density_gumbel(0,0,1))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see, already for \\(n=10\\) there is good fit between the distribution of \\(Y_n - \\log n\\) and \\(\\operatorname{Gumbel}(0, 1)\\).\nIn the limit theorem, we had to subtract \\(\\log n\\) from the maximum. Intuitively, this is necessary to ensure that the maximum does not diverge to \\(\\infty\\), so we obtain an actual distribution in the limit. This is similar to subtracting the mean in the central limit theorem (CLT).\nSimilar to the CLT, the limit theorem holds for a much larger class of distributions, not just exponential distributions and similar to the CLT we have to stabilize the maxima to obtain a valid limit. The next theorem makes this precise.\n\n\n\n\n\n\nTheorem (limit theorem for extrema) (Johnson, Kotz, and Balakrishnan 1995)\n\n\n\nLet \\(X_i, i \\in \\mathbf N\\) be a sequence of i.i.d. random variables, let \\(Y_n = \\max_{i = 1, \\dots, n} X_i\\) be the running maximum and consider \\[\n    Z_{n} = Y_{n} - b_{n},\n\\] for a sequence of real numbers \\(b_n\\), such that for every \\(k\\) \\(b_{kn} - b_n\\) converges as \\(n\\to \\infty\\) for every \\(k\\).2\nIf \\(Z_n\\) converges in distribution to a distribution with an injective CDF3, then the limiting distribution is a Gumbel distribution.\n\n\n3 again, could not get rid of this either2 I could not get rid of this technical assumption, see the sidenotes in the proof.\n\n\n\n\n\nProof\n\n\n\n\n\nI basically follow (Johnson, Kotz, and Balakrishnan 1995) in this proof. Let \\(G\\) be the CDF of the limiting distribution and \\(Z \\sim G\\) and denote general CDFs by \\(F\\). We have to show that \\[\n    G(x) = \\exp \\left( - \\exp \\left( -\\frac{x - \\mu}{\\beta} \\right) \\right)\n\\] for some \\(\\mu \\in \\mathbf R\\) and \\(\\beta \\in \\mathbf R_{&gt; 0}\\). Let \\(k\\in\\mathbf N\\) and partition the random variables into blocks of size \\(n\\), and consider the block-wise maximum, i.e.\n\\[\n    \\begin{align*}\n        Y^{j}_{n} = \\max_{i = 1, \\dots, n} X_{(j - 1)k + i} && j = 1, \\dots, k.\n    \\end{align*}\n\\] Let \\(Z^j_n = Y^j_n - b_n\\) and let \\(n \\to \\infty\\). Then \\(Z^j_n \\stackrel{\\mathcal D}{\\longrightarrow} Z\\) for \\(j = 1, \\dots, k\\).\nNow \\(Z_{kn} = \\max_{j = 1, \\dots, k} Z^j_n + b_{n} - b_{kn}\\) also converges to \\(Z\\) in distribution. By the properties of the CDF and the i.i.d. assumption, we have\n\\[\n    F_{Z_{kn}} (z) = \\mathbf P \\left( \\max_{j = 1, \\dots k} Z^{j}_n \\leq z - b_{n} + b_{kn} \\right)  = (F_{Z^{1}_n}(z - b_{n} + b_{kn}))^{k}.\n\\] As \\(n\\) goes to \\(\\infty\\), the left hand side converges to \\(G(z)\\), and so the right-hand side does as well. Assuming \\(-b_n + b_{kn} \\to c_{k}\\) as \\(n\\to \\infty\\), the right-hand side converges to \\(G(z - c_k)^k\\) as well. 4 Thus \\[\n    G(z) = G(z - c_{k})^{k},\n\\] or, equivalently, \\[\n    G(z + c_{k}) = G(z)^{k}.\n\\] This implies \\[\n    G(z + c_{k} + c_{l}) = (G(z)^{k})^{l} = G(z)^{kl} = G(z + c_{kl}),\n\\] so \\(c_k + c_l = c_{kl}\\), if \\(G\\) is injective 5. Thus \\(c_k = \\xi\\log k\\) for some \\(\\xi \\in \\mathbf R\\).\nTaking logs twice, we obtain \\[\n    \\log k + \\log (- \\log G(z)) = \\log - \\log G(z + \\xi \\log k),\n\\] and so \\(z \\mapsto \\log (- \\log G(z))\\) is an affine function, which was just what we had to show.\n\n\n\n5 again, (Johnson, Kotz, and Balakrishnan 1995) skip over this4 (Johnson, Kotz, and Balakrishnan 1995) as well as wikipedia directly use \\(-b_n + b_{kn} = b_k\\), but I haven’t seen a direct proof yet. (Haan and Ferreira 2006) goes a different, more precise route.Finally, to check whether this works, let us perform some simulations for the maximum of \\(n\\) standard normal draws.\n\n\nCode\nfrom scipy.stats.distributions import norm\n\nfig, axs = plt.subplots(3,2, figsize=(15,5))\nfig.tight_layout()\nns = [2, 4, 10, 20, 50, 100]\nm = 100000\n\nfor ax, n in zip(axs.flatten(), ns):\n    X = np.random.normal(0, 1, (m, n))\n    Y = np.max(X, axis = 1)\n    min_z, max_z = min(Y), max(Y)\n    ax.hist(Y, bins=100, density=True)\n    mu = norm.ppf(1-1/n)\n    beta = norm.ppf(1 - 1 / n / np.exp(1)) - mu\n    ax.plot(np.linspace(min_z, max_z, 1000), density_gumbel(np.linspace(min_z, max_z, 1000), mu, beta), 'r')\n    ax.set_title(f\"max of {n} standard normal draws\")\n    # mode of gumbel distribution is at mu\n    ax.set_ylim(0, 1.2* density_gumbel(mu,mu,beta))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHaan, L. de, and Ana Ferreira. 2006. Extreme Value Theory: An Introduction. Springer Series in Operations Research. New York ; London: Springer.\n\n\nJohnson, Norman L., Samuel Kotz, and Narayanaswamy Balakrishnan. 1995. Continuous Univariate Distributions. Vol. II. John Wiley & Sons, Ltd."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "This is me, Stefan Heyder. I am currently finishing my PhD degree at Technische Universität Ilmenau.\nI will sporadically use this blog to process my thoughts on computational statistics. This might include procrastination on my PhD thesis."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Brockhaus, E.K., Wolffram, D., Stadler, T., Osthege, M., Mitra, T., Littek, J.M., Krymova, E., Klesen, A.J., Huisman, J.S., Heyder, S., et al. (2023). Why are different estimates of the effective reproductive number so different? A case study on COVID-19 in Germany. PLOS Computational Biology 19, e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\nHeyder, S., and Hotz, T. (2023). Measures of COVID-19 Spread. In Covid-19 pandisziplinär und international: Gesundheitswissenschaftliche, gesellschaftspolitische und philosophische Hintergründe Medizin, Kultur, Gesellschaft., A. Kraemer and M. Medzech, eds. (Springer Fachmedien), pp. 51–66. https://doi.org/10.1007/978-3-658-40525-0_3.\nGroßmann, M., Bohm, S., Heyder, S., Schwarzburg, K., Kleinschmidt, P., Runge, E., and Hannappel, T. (2023). Generalized Modeling of Photoluminescence Transients. Physica Status Solidi (b) 260, 2200339. https://doi.org/10.1002/pssb.202200339.\nYeo, Y.L., Kirlangic, M.E., Heyder, S., Supriyanto, E., Mohamad Salim, M.I., Fiedler, P., and Haueisen, J. (2023). Linear versus Quadratic Detrending in Analyzing Simultaneous Changes in DC-EEG and Transcutaneous pCO2. In 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (IEEE), pp. 1–4. https://doi.org/10.1109/EMBC40787.2023.10340855.\n\n\n\n\n\nSherratt, K., Gruson, H., Grah, R., Johnson, H., Niehus, R., Prasse, B., Sandman, F., Deuschel, J., Wolffram, D., Abbott, S., et al. (2022). Predictive performance of multi-model ensemble forecasts of COVID-19 across European nations. Preprint at Epidemiology, https://doi.org/10.1101/2022.06.16.22276024.\nBracher, J., Wolffram, D., Deuschel, J., Görgen, K., Ketterer, J.L., Ullrich, A., Abbott, S., Barbarossa, M.V., Bertsimas, D., Bhatia, S., et al. (2022). National and subnational short-term forecasting of COVID-19 in Germany and Poland during early 2021. Commun Med 2, 1–17. https://doi.org/10.1038/s43856-022-00191-8.\nGrundel, S., Heyder, S., Hotz, T., Ritschel, T.K.S., Sauerteig, P., and Worthmann, K. (2022). How Much Testing and Social Distancing is Required to Control COVID-19? Some Insight Based on an Age-Differentiated Compartmental Model. SIAM J. Control Optim. 60, S145–S169. https://doi.org/10.1137/20M1377783.\nXie, T., Köhler, M., Heyder, S., Günther, M., and Cao, J. (2022). Microfluidically-Assisted Isolation and Characterization of Achromobacter spanius from Soils for Microbial Degradation of Synthetic Polymers and Organic Solvents. Environments 9, 147. https://doi.org/10.3390/environments9120147.\n\n\n\n\n\nBurgard, J.P., Heyder, S., Hotz, T., and Krueger, T. (2021). Regional estimates of reproduction numbers with application to COVID-19. arXiv:2108.13842 [stat] (accepted for publication).\nGrundel, S.M., Heyder, S., Hotz, T., Ritschel, T.K.S., Sauerteig, P., and Worthmann, K. (2021). How to Coordinate Vaccination and Social Distancing to Mitigate SARS-CoV-2 Outbreaks. SIAM J. Appl. Dyn. Syst. 20, 1135–1157. https://doi.org/10.1137/20M1387687.\nBracher, J., Wolffram, D., Deuschel, J., Görgen, K., Ketterer, J.L., Ullrich, A., Abbott, S., Barbarossa, M.V., Bertsimas, D., Bhatia, S., et al. (2021). A pre-registered short-term forecasting study of COVID-19 in Germany and Poland during the second wave. Nat Commun 12, 5173. https://doi.org/10.1038/s41467-021-25207-0.\n\n\n\n\n\nHotz, T., Glock, M., Heyder, S., Semper, S., Böhle, A., and Krämer, A. (2020). Monitoring the spread of COVID-19 by estimating reproduction numbers over time. arXiv:2004.08557 [q-bio, stat].\nMiolane, N., Guigui, N., Brigant, A.L., Mathe, J., Hou, B., Thanwerdas, Y., Heyder, S., Peltre, O., Koep, N., Zaatiti, H., et al. (2020). Geomstats: A Python Package for Riemannian Geometry in Machine Learning. Journal of Machine Learning Research 21, 1–9."
  },
  {
    "objectID": "own_publications.html",
    "href": "own_publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Issued - Oldest\n        \n         \n          Issued - Newest\n        \n         \n          Title\n        \n         \n          Journal\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nIssued\n\n\nTitle\n\n\nJournal\n\n\n\n\n\n\nJanuary,2023\n\n\nGeneralized Modeling of Photoluminescence Transients\n\n\nPhysica Status Solidi (b)\n\n\n\n\nNovember,2022\n\n\nMicrofluidically-Assisted Isolation and Characterization of Achromobacter spanius from Soils for Microbial Degradation of Synthetic Polymers and Organic Solvents\n\n\nEnvironments\n\n\n\n\nAugust,2021\n\n\nRegional estimates of reproduction numbers with application to COVID-19\n\n\n \n\n\n\n\nAugust,2021\n\n\nA pre-registered short-term forecasting study of COVID-19 in Germany and Poland during the second wave\n\n\nNature Communications\n\n\n\n\nJanuary,1970\n\n\nMeasures of COVID-19 Spread\n\n\n“Covid-19 pandisziplinär und international:\n\n\n\n\nJanuary,1970\n\n\nGeomstats: A Python Package for Riemannian Geometry in Machine Learning\n\n\nJournal of Machine Learning Research\n\n\n\n\nNovember,2023\n\n\nWhy are different estimates of the effective reproductive number so different? A case study on COVID-19 in Germany\n\n\nPLOS Computational Biology\n\n\n\n\nJuly,2023\n\n\nLinear versus Quadratic Detrending in Analyzing Simultaneous Changes in DC-EEG and Transcutaneous pCO2\n\n\n2023 45th Annual International Conference of the IEEE\n\n\n\n\nOctober,2022\n\n\nNational and subnational short-term forecasting of COVID-19 in Germany and Poland during early 2021\n\n\nCommunications Medicine\n\n\n\n\nJune,2022\n\n\nPredictive performance of multi-model ensemble forecasts of COVID-19 across European nations\n\n\n \n\n\n\n\nApril,2022\n\n\nHow Much Testing and Social Distancing is Required to Control COVID-19? Some Insight Based on an Age-Differentiated Compartmental Model\n\n\nSIAM Journal on Control and Optimization\n\n\n\n\nJanuary,2021\n\n\nHow to Coordinate Vaccination and Social Distancing to Mitigate SARS-CoV-2 Outbreaks\n\n\nSIAM Journal on Applied Dynamical Systems\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html#publications",
    "href": "publications.html#publications",
    "title": "Statistical Ramblings",
    "section": "",
    "text": "Bracher, J., Wolffram, D., Deuschel, J., Görgen, K., Ketterer, J.L., Ullrich, A., Abbott, S., Barbarossa, M.V., Bertsimas, D., Bhatia, S., et al. (2021). A pre-registered short-term forecasting study of COVID-19 in Germany and Poland during the second wave. Nat Commun 12, 5173. https://doi.org/10.1038/s41467-021-25207-0.\nGrundel, S., Heyder, S., Hotz, T., Ritschel, T.K.S., Sauerteig, P., and Worthmann, K. (2022). How Much Testing and Social Distancing is Required to Control COVID-19? Some Insight Based on an Age-Differentiated Compartmental Model. SIAM J. Control Optim. 60, S145–S169. https://doi.org/10.1137/20M1377783.\nGrundel, S.M., Heyder, S., Hotz, T., Ritschel, T.K.S., Sauerteig, P., and Worthmann, K. (2021). How to Coordinate Vaccination and Social Distancing to Mitigate SARS-CoV-2 Outbreaks. SIAM J. Appl. Dyn. Syst. 20, 1135–1157. https://doi.org/10.1137/20M1387687.\nHeyder, S., and Hotz, T. (2023). Measures of COVID-19 Spread. In Covid-19 pandisziplinär und international: Gesundheitswissenschaftliche, gesellschaftspolitische und philosophische Hintergründe Medizin, Kultur, Gesellschaft., A. Kraemer and M. Medzech, eds. (Springer Fachmedien), pp. 51–66. https://doi.org/10.1007/978-3-658-40525-0_3.\nHotz, T., Glock, M., Heyder, S., Semper, S., Böhle, A., and Krämer, A. (2020). Monitoring the spread of COVID-19 by estimating reproduction numbers over time. arXiv:2004.08557 [q-bio, stat].\nBracher, J., Wolffram, D., Deuschel, J., Görgen, K., Ketterer, J.L., Ullrich, A., Abbott, S., Barbarossa, M.V., Bertsimas, D., Bhatia, S., et al. (2022). National and subnational short-term forecasting of COVID-19 in Germany and Poland during early 2021. Commun Med 2, 1–17. https://doi.org/10.1038/s43856-022-00191-8.\nSherratt, K., Gruson, H., Grah, R., Johnson, H., Niehus, R., Prasse, B., Sandman, F., Deuschel, J., Wolffram, D., Abbott, S., et al. (2022). Predictive performance of multi-model ensemble forecasts of COVID-19 across European nations. Preprint at Epidemiology, https://doi.org/10.1101/2022.06.16.22276024 https://doi.org/10.1101/2022.06.16.22276024.\nBurgard, J.P., Heyder, S., Hotz, T., and Krueger, T. (2021). Regional estimates of reproduction numbers with application to COVID-19. arXiv:2108.13842 [stat] (accepted for publication).\nBrockhaus, E.K., Wolffram, D., Stadler, T., Osthege, M., Mitra, T., Littek, J.M., Krymova, E., Klesen, A.J., Huisman, J.S., Heyder, S., et al. (2023). Why are different estimates of the effective reproductive number so different? A case study on COVID-19 in Germany. PLOS Computational Biology 19, e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\nGroßmann, M., Bohm, S., Heyder, S., Schwarzburg, K., Kleinschmidt, P., Runge, E., and Hannappel, T. (2023). Generalized Modeling of Photoluminescence Transients. Physica Status Solidi (b) 260, 2200339. https://doi.org/10.1002/pssb.202200339.\nYeo, Y.L., Kirlangic, M.E., Heyder, S., Supriyanto, E., Mohamad Salim, M.I., Fiedler, P., and Haueisen, J. (2023). Linear versus Quadratic Detrending in Analyzing Simultaneous Changes in DC-EEG and Transcutaneous pCO2. In 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (IEEE), pp. 1–4. https://doi.org/10.1109/EMBC40787.2023.10340855.\nXie, T., Köhler, M., Heyder, S., Günther, M., and Cao, J. (2022). Microfluidically-Assisted Isolation and Characterization of Achromobacter spanius from Soils for Microbial Degradation of Synthetic Polymers and Organic Solvents. Environments 9, 147. https://doi.org/10.3390/environments9120147.\nMiolane, N., Guigui, N., Brigant, A.L., Mathe, J., Hou, B., Thanwerdas, Y., Heyder, S., Peltre, O., Koep, N., Zaatiti, H., et al. (2020). Geomstats: A Python Package for Riemannian Geometry in Machine Learning. Journal of Machine Learning Research 21, 1–9."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Optimal Gaussian Importance Sampling for Bayesian Inverse Problems on August 12 2024 @ Bernoulli-ims 11th World Congress in Probability and Statistics, Bochum, Germany\nExploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling on February 18 2024 @ Kolloquium of SFB 1294: Data Assimilation, Potsdam, Germany, joint work with Thomas Hotz"
  },
  {
    "objectID": "talks.html#section",
    "href": "talks.html#section",
    "title": "Talks",
    "section": "",
    "text": "Optimal Gaussian Importance Sampling for Bayesian Inverse Problems on August 12 2024 @ Bernoulli-ims 11th World Congress in Probability and Statistics, Bochum, Germany\nExploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling on February 18 2024 @ Kolloquium of SFB 1294: Data Assimilation, Potsdam, Germany, joint work with Thomas Hotz"
  },
  {
    "objectID": "talks.html#section-1",
    "href": "talks.html#section-1",
    "title": "Talks",
    "section": "2023",
    "text": "2023\n\nExploiting independence in Gaussian importance sampling for Bayesian inverse problems on December 15 2023 @ CMStatistics, Berlin, Germany\nPartially Gaussian State Space Models for Epidemiological Modelling on September 14 2023 @ Kolloquium KIT, Karlsruhe, Germany, joint work with Thomas Hotz\nImproving short term forecasts of COVID-19 incidence with subnational epidemic indicators on September 14 2023 @ CEN 2023, Basel, Switzerland, joint work with Thomas Hotz\nRegional estimates of epidemic growth factors with application to COVID-19 on March 08 2023 @ GPSD 2023, Essen, Germany, joint work with Jan Pablo Burgard, Tyll Krüger, Thomas Hotz\nModelling and Control of the COVID-19 Epidemic on January 25 2023 @ Wissenschaftsforum TU Ilmenau, Ilmenau, Germany, joint work with Sara Grundel, Thomas Hotz, Tobias Ritschel, Philipp Sauerteig, Karl Worthmann"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "",
    "text": "Brockhaus, E.K., Wolffram, D., Stadler, T., Osthege, M., Mitra, T., Littek, J.M., Krymova, E., Klesen, A.J., Huisman, J.S., Heyder, S., et al. (2023). Why are different estimates of the effective reproductive number so different? A case study on COVID-19 in Germany. PLOS Computational Biology 19, e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\nHeyder, S., and Hotz, T. (2023). Measures of COVID-19 Spread. In Covid-19 pandisziplinär und international: Gesundheitswissenschaftliche, gesellschaftspolitische und philosophische Hintergründe Medizin, Kultur, Gesellschaft., A. Kraemer and M. Medzech, eds. (Springer Fachmedien), pp. 51–66. https://doi.org/10.1007/978-3-658-40525-0_3.\nGroßmann, M., Bohm, S., Heyder, S., Schwarzburg, K., Kleinschmidt, P., Runge, E., and Hannappel, T. (2023). Generalized Modeling of Photoluminescence Transients. Physica Status Solidi (b) 260, 2200339. https://doi.org/10.1002/pssb.202200339.\nYeo, Y.L., Kirlangic, M.E., Heyder, S., Supriyanto, E., Mohamad Salim, M.I., Fiedler, P., and Haueisen, J. (2023). Linear versus Quadratic Detrending in Analyzing Simultaneous Changes in DC-EEG and Transcutaneous pCO2. In 2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) (IEEE), pp. 1–4. https://doi.org/10.1109/EMBC40787.2023.10340855."
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "",
    "text": "Hotz, T., Glock, M., Heyder, S., Semper, S., Böhle, A., and Krämer, A. (2020). Monitoring the spread of COVID-19 by estimating reproduction numbers over time. arXiv:2004.08557 [q-bio, stat].\nMiolane, N., Guigui, N., Brigant, A.L., Mathe, J., Hou, B., Thanwerdas, Y., Heyder, S., Peltre, O., Koep, N., Zaatiti, H., et al. (2020). Geomstats: A Python Package for Riemannian Geometry in Machine Learning. Journal of Machine Learning Research 21, 1–9."
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "",
    "text": "Burgard, J.P., Heyder, S., Hotz, T., and Krueger, T. (2021). Regional estimates of reproduction numbers with application to COVID-19. arXiv:2108.13842 [stat] (accepted for publication).\nGrundel, S.M., Heyder, S., Hotz, T., Ritschel, T.K.S., Sauerteig, P., and Worthmann, K. (2021). How to Coordinate Vaccination and Social Distancing to Mitigate SARS-CoV-2 Outbreaks. SIAM J. Appl. Dyn. Syst. 20, 1135–1157. https://doi.org/10.1137/20M1387687.\nBracher, J., Wolffram, D., Deuschel, J., Görgen, K., Ketterer, J.L., Ullrich, A., Abbott, S., Barbarossa, M.V., Bertsimas, D., Bhatia, S., et al. (2021). A pre-registered short-term forecasting study of COVID-19 in Germany and Poland during the second wave. Nat Commun 12, 5173. https://doi.org/10.1038/s41467-021-25207-0."
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "",
    "text": "Sherratt, K., Gruson, H., Grah, R., Johnson, H., Niehus, R., Prasse, B., Sandman, F., Deuschel, J., Wolffram, D., Abbott, S., et al. (2022). Predictive performance of multi-model ensemble forecasts of COVID-19 across European nations. Preprint at Epidemiology, https://doi.org/10.1101/2022.06.16.22276024.\nBracher, J., Wolffram, D., Deuschel, J., Görgen, K., Ketterer, J.L., Ullrich, A., Abbott, S., Barbarossa, M.V., Bertsimas, D., Bhatia, S., et al. (2022). National and subnational short-term forecasting of COVID-19 in Germany and Poland during early 2021. Commun Med 2, 1–17. https://doi.org/10.1038/s43856-022-00191-8.\nGrundel, S., Heyder, S., Hotz, T., Ritschel, T.K.S., Sauerteig, P., and Worthmann, K. (2022). How Much Testing and Social Distancing is Required to Control COVID-19? Some Insight Based on an Age-Differentiated Compartmental Model. SIAM J. Control Optim. 60, S145–S169. https://doi.org/10.1137/20M1377783.\nXie, T., Köhler, M., Heyder, S., Günther, M., and Cao, J. (2022). Microfluidically-Assisted Isolation and Characterization of Achromobacter spanius from Soils for Microbial Degradation of Synthetic Polymers and Organic Solvents. Environments 9, 147. https://doi.org/10.3390/environments9120147."
  },
  {
    "objectID": "talks.html#optimal-gaussian-importance-sampling-for-bayesian-inverse-problems---2024-02-18---exploiting-independence-for-optimal-gaussian-importance-sampling---with-application-to-spatio-temporal-epidemiological-modelling",
    "href": "talks.html#optimal-gaussian-importance-sampling-for-bayesian-inverse-problems---2024-02-18---exploiting-independence-for-optimal-gaussian-importance-sampling---with-application-to-spatio-temporal-epidemiological-modelling",
    "title": "Talks",
    "section": "2024- 2024-08-12 - Optimal Gaussian Importance Sampling for Bayesian Inverse Problems - 2024-02-18 - Exploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling ]",
    "text": "2024- 2024-08-12 - Optimal Gaussian Importance Sampling for Bayesian Inverse Problems - 2024-02-18 - Exploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling ]\nNULL"
  },
  {
    "objectID": "talks.html#optimal-gaussian-importance-sampling-for-bayesian-inverse-problems---2024-02-18---exploiting-independence-for-optimal-gaussian-importance-sampling---with-application-to-spatio-temporal-epidemiological-modelling-1",
    "href": "talks.html#optimal-gaussian-importance-sampling-for-bayesian-inverse-problems---2024-02-18---exploiting-independence-for-optimal-gaussian-importance-sampling---with-application-to-spatio-temporal-epidemiological-modelling-1",
    "title": "Talks",
    "section": "2024- 2024-08-12 - Optimal Gaussian Importance Sampling for Bayesian Inverse Problems - 2024-02-18 - Exploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling [[1]]",
    "text": "2024- 2024-08-12 - Optimal Gaussian Importance Sampling for Bayesian Inverse Problems - 2024-02-18 - Exploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling [[1]]\nNULL"
  },
  {
    "objectID": "talks.html#section-2",
    "href": "talks.html#section-2",
    "title": "Talks",
    "section": "2022",
    "text": "2022\n\nRegional estimates of reproduction numbers with application to COVID-19 on June 30 2022 @ IMS 2022, London, UK, joint work with Jan Pablo Burgard, Tyll Krüger, Thomas Hotz\nRegional estimates of reproduction numbers with application to COVID-19 on March 31 2022 @ DAGStat 2022, Hamburg, Germany, joint work with Jan Pablo Burgard, Tyll Krüger, Thomas Hotz"
  },
  {
    "objectID": "talks.html#section-3",
    "href": "talks.html#section-3",
    "title": "Talks",
    "section": "2019",
    "text": "2019\n\nNon-asymptotic confidence sets for Frechet means on July 22 2019 @ EMS 2019, Palermo, Italy, joint work with Thomas Hotz"
  },
  {
    "objectID": "talks.html#section-4",
    "href": "talks.html#section-4",
    "title": "Talks",
    "section": "2018",
    "text": "2018\n\nBenfords Gesetz on October 26 2018 @ Kinderuni TU Ilmenau, Ilmenau, Germany, joint work with Thomas Hotz\nKalman filter on Lie groups on June 26 2018 @ S4G 2018, Prague, Czech Republic, joint work with Thomas Hotz"
  }
]