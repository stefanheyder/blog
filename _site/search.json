[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "This is me, Stefan Heyder. I am currently finishing my PhD degree at Technische Universität Ilmenau.\nI will sporadically use this blog to process my thoughts on computational statistics. This might include procrastination on my PhD thesis."
  },
  {
    "objectID": "posts/fisherian-reduction-t-Test/index.html",
    "href": "posts/fisherian-reduction-t-Test/index.html",
    "title": "Fisherian reduction for the t-Test",
    "section": "",
    "text": "In the second chapter of (Cox 2006) the authors talks about a Fisherian reduction which I think of as a framework of doing inference given a sufficient statistic \\(S\\). An interesting point here is that one can use the conditional distribution of the data, \\(X_1, \\dots, X_n\\) say, on \\(S\\) to evaluate the fit of the model.\nIn this post I want to explore this concept in the setting of a standard \\(t\\)-Test, i.e. we have \\(X_i \\overset{\\text{i.i.d}}{\\sim} \\mathcal N(\\mu, \\sigma^2)\\). The parameter of interest is of course \\(\\mu\\) and \\(S = (\\bar X_n, \\hat\\sigma^2_{n})\\) is a sufficient statistic, with \\(\\hat\\sigma^2_n\\) the empirical variance.\nTo apply the Fisherian reduction we thus need to find the conditional distribution of \\(X = \\left(X_1, \\dots, X_n\\right)\\) on \\(\\bar X_n\\), i.e.\\(X | \\bar X_n\\). For this, let \\(A = \\left(A_1, B\\right)  \\in \\mathbf R^{n\\times n}\\) be an orthogonal matrix whose first column is \\[A_1 = \\left(\\frac 1 {\\sqrt{n}}, \\dots , \\frac 1 {\\sqrt{n}} \\right).\\]\nThen \\(Y = A^TX \\sim \\mathcal N \\left(\\mu A^T\\mu \\mathbf 1, \\sigma^2 I_{n}\\right)\\) and \\(Y = \\left(\\sqrt{n} \\bar X_n, Z\\right)\\) where \\(Z \\sim \\mathcal N \\left(\\sigma^2 I_{n - 1}\\right)\\), \\(Z\\) and \\(\\bar X_n\\) being independent.\nTransforming back we obtain the conditional distribution we sought: \\[ X | \\bar X_n \\sim AY | \\bar X_n \\mathbf 1 \\sim \\bar X_n + B Z\\]\nThe catch here is that \\(B\\) is a \\(n\\times (n - 1)\\) dimensional matrix, so \\(X | \\bar X_n\\) has a normal distribution of dimension \\(n-1\\), e.g. the variance-covariance matrix is rank-deficient.\nLet’s verify this through simulation.\n\n\nCode\nmu &lt;- 10\nsigma &lt;- 1\nn &lt;- 2\nm &lt;- 1000\nx &lt;- matrix(rnorm(n * m, mu, sigma), nrow= n)\n\ny &lt;- t(t(x) - colMeans(x))\n\ntransformed &lt;- c(matrix(c(1/sqrt(2), -1/sqrt(2)), nrow = 1) %*% y)\n\nplot(y[1,], y[2,])\n\nhist(transformed)\nprint(mean(transformed))\nprint(sd(transformed))\n\n\n\n\n\n\n\n\n\n[1] 0.007553103\n[1] 1.05018\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCox, D. R. 2006. Principles of Statistical Inference. Cambridge ; New York: Cambridge University Press."
  },
  {
    "objectID": "posts/regularity-conditions/index.html",
    "href": "posts/regularity-conditions/index.html",
    "title": "Asymptotics of estimators",
    "section": "",
    "text": "In my PhD thesis, I compare two methods to perform optimal importance sampling: the Cross-Entropy method (CE) and Efficient Importance Sampling (EIS). There are several criteria that you may want to consider for this comparison, but without going into too much detail , in this post, I’ll focus on only one aspect: asymptotics, in particular asymptotic relative efficiencies.If you’re interested, check out the draft of my thesis on Github.\nIn the context of my thesis, this crops up as both methods are simulation-based. Usually, we have to resort to simulation techniques when analytical computation is too difficult, which is exactly the case here! Both methods actually want to solve an optimization problem in the background, solving for an optimal parameter \\(\\psi\\). Unfortunately, the optimization problems involve quantities that we have no (analytical) access to. We can, however, set up a simulation routine that provides an estimate \\(\\hat\\psi\\) of this optimal parameter.\nBut using simulations means that the output of both methods is now random, i.e. we can expect to get different results \\(\\hat\\psi\\) when we repeatedly apply the methods using different RNG seeds. This can be problematic: if there is too much variation the actual \\(\\hat\\psi\\) we obtain could be far away from the optimal \\(\\psi\\) and the performance of the methods suffers.\nThis is where asymptotics come into play. When statisticians talk about asymptotics for estimators, here \\(\\hat\\psi\\), they are usually concerned with two things: consistency and asymptotic normality. Consistency is a critical property of an estimator."
  },
  {
    "objectID": "posts/regularity-conditions/index.html#consistency",
    "href": "posts/regularity-conditions/index.html#consistency",
    "title": "Asymptotics of estimators",
    "section": "consistency",
    "text": "consistency\n\n\n\n\n\n\nDefinition (consistency)\n\n\n\nLet \\((\\hat\\psi_{N})_{N \\in \\mathbf N}\\) be a sequence of estimator of \\(\\psi\\). We say that \\(\\hat\\psi_{N}\\) is weakly consistent if \\[\n\\hat\\psi_{N} \\stackrel{\\mathbb P}{\\to} \\psi\n\\] and strongly consistent, if \\[\n\\hat \\psi_{N} \\stackrel{\\text{a.s.}}{\\to} \\psi.\n\\]\n\n\nIn the following I’ll be a little bit sloppy and identify \\(\\hat\\psi_{N}\\) with the sequence of estimators \\((\\hat\\psi_{N})_{N \\in \\mathbf N}\\).Consistency captures the intuitive property that, as we increase the number of samples \\(N\\) used in our simulation routine, the noisy estimate \\(\\hat\\psi_N\\) should get closer and closer to the true \\(\\psi\\). Without consistency, our simulation routine is essentially useless.\nBut consistency alone does not tell us how large we should choose our sample size \\(N\\). For that, central limit theorems can help.\n1"
  },
  {
    "objectID": "posts/regularity-conditions/index.html#asymptotic-normality",
    "href": "posts/regularity-conditions/index.html#asymptotic-normality",
    "title": "Asymptotics of estimators",
    "section": "Asymptotic normality",
    "text": "Asymptotic normality\n\n\n\n\n\n\nDefinition (central limit theorem)\n\n\n\nAgain, let \\((\\hat\\psi_{N})_{N \\in \\mathbf N}\\) be a sequence of estimator of \\(\\psi\\). If \\[\n\\sqrt {N} \\left( \\hat \\psi_N - \\psi \\right) \\stackrel{\\mathcal D}{\\to} \\mathcal N(0, \\Sigma),\n\\] we say that \\(\\hat\\psi_{N}\\) fulfills a central limit theorem with asymptotic covariance matrix \\(\\Sigma\\).\n\n\nIf we have a consistent estimator, notice that \\(\\hat\\psi_N - \\psi\\) goes to \\(0\\) as \\(N\\) goes to \\(\\infty\\) (in probability or almost surely, depending on the type of consistency). Multiplying by \\(\\sqrt{N}\\) “blows up” this error, essentially zooming in to what happens around the true value \\(\\psi\\). Note that any estimator that fulfills a central limit theorem is weakly consistent.\nWhy should we assume a normal distribution as the limiting distribution? At first this choice may seem surprising, maybe even restrictive. But it turns out that for large classes of estimators, e.g. M- and Z-estimators, we can proof such a central limit theorem.\nHow does having a central limit theorem help us? If we know \\(\\Sigma\\), we can use it get a heuristic on how large we should choose \\(N\\). Using the approximation \\[\n\\hat\\psi_{N} -\\psi \\approx \\mathcal N\\left(0, \\frac{\\Sigma}{N}\\right)\n\\] we can choose \\(N\\) such that, e.g., for a given error \\(\\varepsilon &gt; 0\\), the probability that \\(\\lVert \\hat \\psi_N - \\psi \\rVert &lt; \\varepsilon\\) becomes, say, at least \\(80\\%\\).\nAdditionally, if we have two estimators \\(\\hat \\psi^1\\) and \\(\\hat\\psi^{2}\\), both of which fulfill a central limit theorem with asymptotic covariance matrices \\(\\Sigma_1\\) and \\(\\Sigma_{2}\\), we can compare \\(\\Sigma_1\\) and \\(\\Sigma_2\\). The estimator with the “smaller” asymptotic covariance matrix is the one we should prefer.\nAs we are dealing with matrices it is not necessarily clear what smaller means, we could be interested in, e.g.\n\n\\(\\Sigma_1 \\succ \\Sigma_{2}\\), i.e. \\(\\Sigma_1 - \\Sigma_2\\) is postive definite, or\n\\(\\operatorname{trace} \\left( \\Sigma_{1} \\right) &gt; \\operatorname{trace} \\left( \\Sigma_{2} \\right)\\), i.e. the asymptotic mean squared error (MSE) is smaller for \\(\\psi_2\\).\n\nUsually the asymptotic covariance matrix \\(\\Sigma\\) is not known, usually because it depends on either the true parameter \\(\\psi\\). For a toy example, consider the normal distribution \\(\\mathcal N(\\mu, \\sigma^2)\\) where the mean \\(\\mu \\in \\mathbf R\\) and variance is \\(\\sigma^2 \\in \\mathbf R_{&gt; 0}\\). Then the sample mean \\(\\bar X_{N}\\) for samples \\(X_{1}, \\dots, X_{N} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal N(\\mu, \\sigma^{2})\\) as an estimator of \\(\\mu\\) fulfills a central limit theorem: \\[\n\\sqrt{N} (\\bar X_{N} - \\mu) \\stackrel{\\mathcal D}\\to \\mathcal N(0, \\sigma^{2}).\n\\] Here \\(\\sigma^2\\) may be unknown. In practice we can estimate it consistently by the sample variance \\(\\hat \\sigma_N^{2}\\), and so \\[\n\\frac{\\sqrt{N}}{\\sqrt{\\hat \\sigma_{N}^2}} \\left( \\bar X_{N} - \\mu \\right) \\stackrel{\\mathcal D}\\to \\mathcal N(0,1),\n\\] by Slutsky’s lemma. Thus, for large \\(N\\), we can the true asymptotic variance \\(\\sigma^2\\) by its consistent estimate \\(\\hat\\sigma_N^2\\), and proceed as above."
  },
  {
    "objectID": "posts/regularity-conditions/index.html#whats-next",
    "href": "posts/regularity-conditions/index.html#whats-next",
    "title": "Asymptotics of estimators",
    "section": "What’s next?",
    "text": "What’s next?\nIn this post, I’ve tried to convey the usefulness of asymptotics, especially when it comes to comparing two estimators \\(\\psi\\). However, I have not given you any guidance on when an estimator is consistent and when it fulfills a central limit theorem. It turns out, that for many types of estimators we can obtain, under some assumptions, central limit theorems. These settings I will discuss in a following post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Asymptotics of estimators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nFisherian reduction for the t-Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nStefan Heyder\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/regularity-conditions/index.html#footnotes",
    "href": "posts/regularity-conditions/index.html#footnotes",
    "title": "Asymptotics of estimators",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2↩︎"
  }
]