[
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "Stefan Heyder\nWiesenweg 21, 98693 Ilmenau\nstefan.heyder@gmail.com"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Brockhaus, Elisabeth K.; Wolffram, Daniel; Stadler, Tanja; Osthege, Michael; Mitra, Tanmay; Littek, Jonas M.; Krymova, Ekaterina; Klesen, Anna J.; Huisman, Jana S.; Heyder, Stefan; Helleckes, Laura M.; Heiden, Matthias an der; Funk, Sebastian; Abbott, Sam; Bracher, Johannes (2023-11-27). Why are different estimates of the effective reproductive number so different? A case study on COVID-19 in Germany. PLOS Computational Biology. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011653.\nHeyder, Stefan; Hotz, Thomas (2023-10-04). Measures of COVID-19 Spread. Covid-19 pandisziplinär und international: Gesundheitswissenschaftliche, gesellschaftspolitische und philosophische Hintergründe. https://doi.org/10.1007/978-3-658-40525-0_3."
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "Publications",
    "section": "",
    "text": "Brockhaus, Elisabeth K.; Wolffram, Daniel; Stadler, Tanja; Osthege, Michael; Mitra, Tanmay; Littek, Jonas M.; Krymova, Ekaterina; Klesen, Anna J.; Huisman, Jana S.; Heyder, Stefan; Helleckes, Laura M.; Heiden, Matthias an der; Funk, Sebastian; Abbott, Sam; Bracher, Johannes (2023-11-27). Why are different estimates of the effective reproductive number so different? A case study on COVID-19 in Germany. PLOS Computational Biology. https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011653.\nHeyder, Stefan; Hotz, Thomas (2023-10-04). Measures of COVID-19 Spread. Covid-19 pandisziplinär und international: Gesundheitswissenschaftliche, gesellschaftspolitische und philosophische Hintergründe. https://doi.org/10.1007/978-3-658-40525-0_3."
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "Publications",
    "section": "2022",
    "text": "2022\n\nBracher, Johannes; Wolffram, Daniel; Deuschel, Jannik; Görgen, Konstantin; Ketterer, Jakob L.; Ullrich, Alexander; Abbott, Sam; Barbarossa, Maria V.; Bertsimas, Dimitris; Bhatia, Sangeeta; Bodych, Marcin; Bosse, Nikos I.; Burgard, Jan Pablo; Castro, Lauren; Fairchild, Geoffrey; Fiedler, Jochen; Fuhrmann, Jan; Funk, Sebastian; Gambin, Anna; Gogolewski, Krzysztof; Heyder, Stefan; Hotz, Thomas; Kheifetz, Yuri; Kirsten, Holger; Krueger, Tyll; Krymova, Ekaterina; Leithäuser, Neele; Li, Michael L.; Meinke, Jan H.; Miasojedow, Błażej; Michaud, Isaac J.; Mohring, Jan; Nouvellet, Pierre; Nowosielski, Jedrzej M.; Ozanski, Tomasz; Radwan, Maciej; Rakowski, Franciszek; Scholz, Markus; Soni, Saksham; Srivastava, Ajitesh; Gneiting, Tilmann; Schienle, Melanie (2022-10-31). National and subnational short-term forecasting of COVID-19 in Germany and Poland during early 2021. Communications Medicine. https://www.nature.com/articles/s43856-022-00191-8.\nSherratt, K.; Gruson, H.; Grah, R.; Johnson, H.; Niehus, R.; Prasse, B.; Sandman, F.; Deuschel, J.; Wolffram, D.; Abbott, S.; Ullrich, A.; Gibson, G.; Ray, El.; Reich, Ng.; Sheldon, D.; Wang, Y.; Wattanachit, N.; Wang, L.; Trnka, J.; Obozinski, G.; Sun, T.; Thanou, D.; Pottier, L.; Krymova, E.; Barbarossa, Mv.; Leithäuser, N.; Mohring, J.; Schneider, J.; Wlazlo, J.; Fuhrmann, J.; Lange, B.; Rodiah, I.; Baccam, P.; Gurung, H.; Stage, S.; Suchoski, B.; Budzinski, J.; Walraven, R.; Villanueva, I.; Tucek, V.; Šmíd, M.; Zajícek, M.; Pérez Álvarez, C.; Reina, B.; Bosse, Ni.; Meakin, S.; Di Loro, P. Alaimo; Maruotti, A.; Eclerová, V.; Kraus, A.; Kraus, D.; Pribylova, L.; Dimitris, B.; Li, Ml.; Saksham, S.; Dehning, J.; Mohr, S.; Priesemann, V.; Redlarski, G.; Bejar, B.; Ardenghi, G.; Parolini, N.; Ziarelli, G.; Bock, W.; Heyder, S.; Hotz, T.; E. Singh, D.; Guzman-Merino, M.; Aznarte, Jl.; Moriña, D.; Alonso, S.; Álvarez, E.; López, D.; Prats, C.; Burgard, Jp.; Rodloff, A.; Zimmermann, T.; Kuhlmann, A.; Zibert, J.; Pennoni, F.; Divino, F.; Català, M.; Lovison, G.; Giudici, P.; Tarantino, B.; Bartolucci, F.; Jona Lasinio, G.; Mingione, M.; Farcomeni, A.; Srivastava, A.; Montero-Manso, P.; Adiga, A.; Hurt, B.; Lewis, B.; Marathe, M.; Porebski, P.; Venkatramanan, S.; Bartczuk, R.; Dreger, F.; Gambin, A.; Gogolewski, K.; Gruziel-Slomka, M.; Krupa, B.; Moszynski, A.; Niedzielewski, K.; Nowosielski, J.; Radwan, M.; Rakowski, F.; Semeniuk, M.; Szczurek, E.; Zielinski, J.; Kisielewski, J.; Pabjan, B.; Holger, K.; Kheifetz, Y.; Scholz, M.; Bodych, M.; Filinski, M.; Idzikowski, R.; Krueger, T.; Ozanski, T.; Bracher, J.; Funk, S. (2022-06-16). Predictive performance of multi-model ensemble forecasts of COVID-19 across European nations. NA. http://medrxiv.org/lookup/doi/10.1101/2022.06.16.22276024.\nGrundel, Sara; Heyder, Stefan; Hotz, Thomas; Ritschel, Tobias K. S.; Sauerteig, Philipp; Worthmann, Karl (2022-04-01). How Much Testing and Social Distancing is Required to Control COVID-19? Some Insight Based on an Age-Differentiated Compartmental Model. SIAM Journal on Control and Optimization. https://epubs.siam.org/doi/10.1137/20M1377783."
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "Publications",
    "section": "2021",
    "text": "2021\n\nBurgard, Jan Pablo; Heyder, Stefan; Hotz, Thomas; Krueger, Tyll (2021-08-31). Regional estimates of reproduction numbers with application to COVID-19. arXiv:2108.13842 [stat] (accepted for publication). http://arxiv.org/abs/2108.13842.\nBracher, J.; Wolffram, D.; Deuschel, J.; Görgen, K.; Ketterer, J. L.; Ullrich, A.; Abbott, S.; Barbarossa, M. V.; Bertsimas, D.; Bhatia, S.; Bodych, M.; Bosse, N. I.; Burgard, J. P.; Castro, L.; Fairchild, G.; Fuhrmann, J.; Funk, S.; Gogolewski, K.; Gu, Q.; Heyder, S.; Hotz, T.; Kheifetz, Y.; Kirsten, H.; Krueger, T.; Krymova, E.; Li, M. L.; Meinke, J. H.; Michaud, I. J.; Niedzielewski, K.; Ożański, T.; Rakowski, F.; Scholz, M.; Soni, S.; Srivastava, A.; Zieliński, J.; Zou, D.; Gneiting, T.; Schienle, M. (2021-08-27). A pre-registered short-term forecasting study of COVID-19 in Germany and Poland during the second wave. Nature Communications. https://www.nature.com/articles/s41467-021-25207-0.\nGrundel, Sara M.; Heyder, Stefan; Hotz, Thomas; Ritschel, Tobias K. S.; Sauerteig, Philipp; Worthmann, Karl (2021-01-01). How to Coordinate Vaccination and Social Distancing to Mitigate SARS-CoV-2 Outbreaks. SIAM Journal on Applied Dynamical Systems. https://epubs.siam.org/doi/abs/10.1137/20M1387687."
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "Publications",
    "section": "2020",
    "text": "2020\n\nMiolane, Nina; Guigui, Nicolas; Brigant, Alice Le; Mathe, Johan; Hou, Benjamin; Thanwerdas, Yann; Heyder, Stefan; Peltre, Olivier; Koep, Niklas; Zaatiti, Hadi; Hajri, Hatem; Cabanes, Yann; Gerald, Thomas; Chauchat, Paul; Shewmake, Christian; Brooks, Daniel; Kainz, Bernhard; Donnat, Claire; Holmes, Susan; Pennec, Xavier (2020-11-01). Geomstats: A Python Package for Riemannian Geometry in Machine Learning. Journal of Machine Learning Research. http://jmlr.org/papers/v21/19-027.html.\nHotz, Thomas; Glock, Matthias; Heyder, Stefan; Semper, Sebastian; Böhle, Anne; Krämer, Alexander (2020-04-18). Monitoring the spread of COVID-19 by estimating reproduction numbers over time. arXiv:2004.08557 [q-bio, stat]. http://arxiv.org/abs/2004.08557."
  },
  {
    "objectID": "drafts/refugees.html",
    "href": "drafts/refugees.html",
    "title": "Data science ramblings",
    "section": "",
    "text": "library(tidyverse)\n\nrefugees &lt;- read_csv(\"population.csv\", skip = 14)\n\nrefugees %&gt;% head()\n\nRows: 33755 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Country of origin, Country of origin (ISO), Country of asylum, Coun...\ndbl (7): Year, Refugees under UNHCR's mandate, Asylum-seekers, IDPs of conce...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nA tibble: 6 × 12\n\n\nYear\nCountry of origin\nCountry of origin (ISO)\nCountry of asylum\nCountry of asylum (ISO)\nRefugees under UNHCR's mandate\nAsylum-seekers\nIDPs of concern to UNHCR\nOther people in need of international protection\nStateless persons\nHost Community\nOthers of concern\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2018\nAfghanistan\nAFG\nAfghanistan\nAFG\n0\n0\n2106893\n-\n0\n0\n489854\n\n\n2018\nBangladesh\nBGD\nAfghanistan\nAFG\n0\n5\n0\n-\n0\n0\n0\n\n\n2018\nIran (Islamic Rep. of)\nIRN\nAfghanistan\nAFG\n34\n9\n0\n-\n0\n0\n0\n\n\n2018\nPakistan\nPAK\nAfghanistan\nAFG\n72194\n121\n0\n-\n0\n0\n0\n\n\n2018\nTajikistan\nTJK\nAfghanistan\nAFG\n0\n7\n0\n-\n0\n0\n0\n\n\n2018\nTürkiye\nTUR\nAfghanistan\nAFG\n0\n139\n0\n-\n0\n0\n0\n\n\n\n\n\n\nrefugees %&gt;%\n    filter(`Country of origin (ISO)` %in% c(\"AFG\", \"SYR\")) %&gt;%\n    filter(Year == max(Year)) %&gt;%\n    group_by(`Country of origin (ISO)`) %&gt;%\n    slice_max(`Refugees under UNHCR's mandate`, n = 3)\n\n\nA grouped_df: 6 × 12\n\n\nYear\nCountry of origin\nCountry of origin (ISO)\nCountry of asylum\nCountry of asylum (ISO)\nRefugees under UNHCR's mandate\nAsylum-seekers\nIDPs of concern to UNHCR\nOther people in need of international protection\nStateless persons\nHost Community\nOthers of concern\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n2023\nAfghanistan\nAFG\nIran (Islamic Rep. of)\nIRN\n3752317\n0\n0\n-\n0\n0\n0\n\n\n2023\nAfghanistan\nAFG\nPakistan\nPAK\n1987717\n61451\n0\n-\n0\n0\n49\n\n\n2023\nAfghanistan\nAFG\nGermany\nDEU\n255077\n44781\n0\n-\n0\n0\n0\n\n\n2023\nSyrian Arab Rep.\nSYR\nTürkiye\nTUR\n3214780\n0\n0\n-\n0\n0\n0\n\n\n2023\nSyrian Arab Rep.\nSYR\nLebanon\nLBN\n784884\n0\n0\n-\n0\n0\n3026\n\n\n2023\nSyrian Arab Rep.\nSYR\nGermany\nDEU\n705812\n72642\n0\n-\n0\n0\n0"
  },
  {
    "objectID": "posts/2025-05-29 data quality/index.html",
    "href": "posts/2025-05-29 data quality/index.html",
    "title": "Early data validation saves you trouble down the line",
    "section": "",
    "text": "Working with poor-quality data sucks. It leads to bugs due to the implicit assumptions we make that turn out to be incorrect. For example:\nIn a recent project we perforemd a lot of ETL tasks on data that comes from an API. These data are then transformed by a Python package we wrote using airflow and sent back to that API.\nHere is the problem: The quality of the data we receive is suboptimal as it is based on input from humans into a web form. To avoid running into problems further down the line, we use a Pydantic model that makes our implicit assumptions explicit. It works well and has saved us a lot of worry. However, every time there is a validation error occurs, we have to write ticket for the data to be fixed, resulting in a lot of unnecessary work (this happens roughly once a week).\nMuch of this could have been avoided, if data validation had taken place much earlier in the process. Although there is validation on the frontend of the web form, there is none on the back end, meaning users can submit anything they want, if they have a JavaScript blocker in place."
  },
  {
    "objectID": "posts/2025-05-29 data quality/index.html#advantages-of-early-data-validation",
    "href": "posts/2025-05-29 data quality/index.html#advantages-of-early-data-validation",
    "title": "Early data validation saves you trouble down the line",
    "section": "advantages of early data validation",
    "text": "advantages of early data validation\nOnce data has passed validation, you have defined an interface it. This means that you can avoid writing many conversion (e.g. string to date, parsing strings to floats, …) and null checks later on, and you can be confident that nothing will break due to such implicit assumptions. If combined with properly typed and type-checked Python code, this goes a long way of avoiding bugs.\nAdditionally, early data validation also enables you to fail early. Let’s say you persist the data in a database (PostrgeSQL in our case) for one reason or another, and then process that data later on. If you later notice that something is fishy with the data, you will have to clean up or invalidate the faulty data in the database.\nThere are probably many more reasons, but these are the ones that I have noticed in our project."
  },
  {
    "objectID": "posts/2025-05-29 data quality/index.html#hurdles-to-implement-early-data-validation",
    "href": "posts/2025-05-29 data quality/index.html#hurdles-to-implement-early-data-validation",
    "title": "Early data validation saves you trouble down the line",
    "section": "hurdles to implement early data validation",
    "text": "hurdles to implement early data validation\nHowever, there may be challenges in implementing these checks early on. In a bigger project, you have to coordinate with everyone that will use the data on a common definition of what a valid data entry is, including technical and non-technical stakeholders.\nIf the data ingestion process is already in place before the goals of data processing are specified it may be hard to add validation later on. This may be due to inertia of the system and also monetary restrictions. Additionally, if validation is put into place after some non-validated data have already entered the system, you now have to distinguish between the new and old data, or clean up the old data."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html",
    "href": "posts/2024-07-07 asymptotics/index.html",
    "title": "Asymptotics of estimators",
    "section": "",
    "text": "In my PhD thesis, I compare two methods to perform optimal importance sampling: the Cross-Entropy method (CE) and Efficient Importance Sampling (EIS). There are several criteria that you may want to consider for this comparison, but without going into too much detail , in this post, I’ll focus on only one aspect: asymptotics, in particular asymptotic relative efficiencies.If you’re interested, check out the draft of my thesis on Github.\nIn the context of my thesis, this crops up as both methods are simulation-based. Usually, we have to resort to simulation techniques when analytical computation is too difficult, which is exactly the case here! Both methods actually want to solve an optimization problem in the background, solving for an optimal parameter \\(\\psi\\). Unfortunately, the optimization problems involve quantities that we have no (analytical) access to. We can, however, set up a simulation routine that provides an estimate \\(\\hat\\psi\\) of this optimal parameter.\nBut using simulations means that the output of both methods is now random, i.e. we can expect to get different results \\(\\hat\\psi\\) when we repeatedly apply the methods using different RNG seeds. This can be problematic: if there is too much variation the actual \\(\\hat\\psi\\) we obtain could be far away from the optimal \\(\\psi\\) and the performance of the methods suffers.\nThis is where asymptotics come into play. When statisticians talk about asymptotics for estimators, here \\(\\hat\\psi\\), they are usually concerned with two things: consistency and asymptotic normality. Consistency is a critical property of an estimator."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html#asymptotic-normality",
    "href": "posts/2024-07-07 asymptotics/index.html#asymptotic-normality",
    "title": "Asymptotics of estimators",
    "section": "Asymptotic normality",
    "text": "Asymptotic normality\n\n\n\n\n\n\nDefinition (central limit theorem)\n\n\n\nAgain, let \\((\\hat\\psi_{N})_{N \\in \\mathbf N}\\) be a sequence of estimator of \\(\\psi\\). If \\[\n\\sqrt {N} \\left( \\hat \\psi_N - \\psi \\right) \\stackrel{\\mathcal D}{\\to} \\mathcal N(0, \\Sigma),\n\\] we say that \\(\\hat\\psi_{N}\\) fulfills a central limit theorem with asymptotic covariance matrix \\(\\Sigma\\).\n\n\nIf we have a consistent estimator, notice that \\(\\hat\\psi_N - \\psi\\) goes to \\(0\\) as \\(N\\) goes to \\(\\infty\\) (in probability or almost surely, depending on the type of consistency). Multiplying by \\(\\sqrt{N}\\) “blows up” this error, essentially zooming in to what happens around the true value \\(\\psi\\). Note that any estimator that fulfills a central limit theorem is weakly consistent.\nWhy should we assume a normal distribution as the limiting distribution? At first this choice may seem surprising, maybe even restrictive. But it turns out that for large classes of estimators, e.g. M- and Z-estimators, we can proof such a central limit theorem.\nHow does having a central limit theorem help us? If we know \\(\\Sigma\\), we can use it get a heuristic on how large we should choose \\(N\\). Using the approximation \\[\n\\hat\\psi_{N} -\\psi \\approx \\mathcal N\\left(0, \\frac{\\Sigma}{N}\\right)\n\\] we can choose \\(N\\) such that, e.g., for a given error \\(\\varepsilon &gt; 0\\), the probability that \\(\\lVert \\hat \\psi_N - \\psi \\rVert &lt; \\varepsilon\\) becomes, say, at least \\(80\\%\\).\nAdditionally, if we have two estimators \\(\\hat \\psi^1\\) and \\(\\hat\\psi^{2}\\), both of which fulfill a central limit theorem with asymptotic covariance matrices \\(\\Sigma_1\\) and \\(\\Sigma_{2}\\), we can compare \\(\\Sigma_1\\) and \\(\\Sigma_2\\). The estimator with the “smaller” asymptotic covariance matrix is the one we should prefer.\nAs we are dealing with matrices it is not necessarily clear what smaller means, we could be interested in, e.g.\n\n\\(\\Sigma_1 \\succ \\Sigma_{2}\\), i.e. \\(\\Sigma_1 - \\Sigma_2\\) is postive definite, or\n\\(\\operatorname{trace} \\left( \\Sigma_{1} \\right) &gt; \\operatorname{trace} \\left( \\Sigma_{2} \\right)\\), i.e. the asymptotic mean squared error (MSE) is smaller for \\(\\psi_2\\).\n\nUsually the asymptotic covariance matrix \\(\\Sigma\\) is not known, usually because it depends on either the true parameter \\(\\psi\\). For a toy example, consider the normal distribution \\(\\mathcal N(\\mu, \\sigma^2)\\) where the mean \\(\\mu \\in \\mathbf R\\) and variance is \\(\\sigma^2 \\in \\mathbf R_{&gt; 0}\\). Then the sample mean \\(\\bar X_{N}\\) for samples \\(X_{1}, \\dots, X_{N} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal N(\\mu, \\sigma^{2})\\) as an estimator of \\(\\mu\\) fulfills a central limit theorem: \\[\n\\sqrt{N} (\\bar X_{N} - \\mu) \\stackrel{\\mathcal D}\\to \\mathcal N(0, \\sigma^{2}).\n\\] Here \\(\\sigma^2\\) may be unknown. In practice we can estimate it consistently by the sample variance \\(\\hat \\sigma_N^{2}\\), and so \\[\n\\frac{\\sqrt{N}}{\\sqrt{\\hat \\sigma_{N}^2}} \\left( \\bar X_{N} - \\mu \\right) \\stackrel{\\mathcal D}\\to \\mathcal N(0,1),\n\\] by Slutsky’s lemma. Thus, for large \\(N\\), we can the true asymptotic variance \\(\\sigma^2\\) by its consistent estimate \\(\\hat\\sigma_N^2\\), and proceed as above."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html#whats-next",
    "href": "posts/2024-07-07 asymptotics/index.html#whats-next",
    "title": "Asymptotics of estimators",
    "section": "What’s next?",
    "text": "What’s next?\nIn this post, I’ve tried to convey the usefulness of asymptotics, especially when it comes to comparing two estimators \\(\\psi\\). However, I have not given you any guidance on when an estimator is consistent and when it fulfills a central limit theorem. It turns out, that for many types of estimators we can obtain, under some assumptions, central limit theorems. These settings I will discuss in a following post."
  },
  {
    "objectID": "posts/2022-02-08 fisherian_reduction/index.html",
    "href": "posts/2022-02-08 fisherian_reduction/index.html",
    "title": "Fisherian reduction for the t-Test",
    "section": "",
    "text": "In the second chapter of (Cox 2006) the authors talks about a Fisherian reduction which I think of as a framework of doing inference given a sufficient statistic \\(S\\). An interesting point here is that one can use the conditional distribution of the data, \\(X_1, \\dots, X_n\\) say, on \\(S\\) to evaluate the fit of the model.\nIn this post I want to explore this concept in the setting of a standard \\(t\\)-Test, i.e. we have \\(X_i \\overset{\\text{i.i.d}}{\\sim} \\mathcal N(\\mu, \\sigma^2)\\). The parameter of interest is of course \\(\\mu\\) and \\(S = (\\bar X_n, \\hat\\sigma^2_{n})\\) is a sufficient statistic, with \\(\\hat\\sigma^2_n\\) the empirical variance.\nTo apply the Fisherian reduction we thus need to find the conditional distribution of \\(X = \\left(X_1, \\dots, X_n\\right)\\) on \\(\\bar X_n\\), i.e.\\(X | \\bar X_n\\). For this, let \\(A = \\left(A_1, B\\right)  \\in \\mathbf R^{n\\times n}\\) be an orthogonal matrix whose first column is \\[A_1 = \\left(\\frac 1 {\\sqrt{n}}, \\dots , \\frac 1 {\\sqrt{n}} \\right).\\]\nThen \\(Y = A^TX \\sim \\mathcal N \\left(\\mu A^T\\mu \\mathbf 1, \\sigma^2 I_{n}\\right)\\) and \\(Y = \\left(\\sqrt{n} \\bar X_n, Z\\right)\\) where \\(Z \\sim \\mathcal N \\left(\\sigma^2 I_{n - 1}\\right)\\), \\(Z\\) and \\(\\bar X_n\\) being independent.\nTransforming back we obtain the conditional distribution we sought: \\[ X | \\bar X_n \\sim AY | \\bar X_n \\mathbf 1 \\sim \\bar X_n + B Z\\]\nThe catch here is that \\(B\\) is a \\(n\\times (n - 1)\\) dimensional matrix, so \\(X | \\bar X_n\\) has a normal distribution of dimension \\(n-1\\), e.g. the variance-covariance matrix is rank-deficient.\nLet’s verify this through simulation.\n\n\nCode\nmu &lt;- 10\nsigma &lt;- 1\nn &lt;- 2\nm &lt;- 1000\nx &lt;- matrix(rnorm(n * m, mu, sigma), nrow= n)\n\ny &lt;- t(t(x) - colMeans(x))\n\ntransformed &lt;- c(matrix(c(1/sqrt(2), -1/sqrt(2)), nrow = 1) %*% y)\n\nplot(y[1,], y[2,])\n\nhist(transformed)\nprint(mean(transformed))\nprint(sd(transformed))\n\n\n\n\n\n\n\n\n\n[1] 0.007553103\n[1] 1.05018\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCox, D. R. 2006. Principles of Statistical Inference. Cambridge ; New York: Cambridge University Press."
  },
  {
    "objectID": "posts/2024-07-17 gumbel_distribution/index.html",
    "href": "posts/2024-07-17 gumbel_distribution/index.html",
    "title": "The Gumbel distribution",
    "section": "",
    "text": "I recently learned about the Gumbel softmax trick, which seemingly allows smooth sampling from a discrete distribution. In writing this post, I want to learn more about the Gumbel distribution that appears in this trick, details on the trick may follow in a separate post.\nThe main component in the softmax trick is the following distribution, due to Gumbel (aptly named after him).\n\n\n\n\n\n\nDefinition (Gumbel distribution)\n\n\n\nFor \\(\\mu \\in \\mathbf R\\) and \\(\\beta \\in \\mathbf R_{&gt; 0}\\) the \\(\\operatorname{Gumbel}(\\mu, \\beta)\\) distribution has the CDF \\[\n\\begin{align*}\n    F: (-\\infty, \\infty) \\to (0, 1) && x \\mapsto \\exp \\left( -\\exp \\left( - \\frac{x - \\mu}{\\beta} \\right)  \\right).\n\\end{align*}\n\\]\n\n\nLet us quickly ensure that this defines a distribution: this CDF is strictly monotonically increasing and for \\(x \\to -\\infty\\) it goes to \\(0\\), while for \\(x \\to \\infty\\) it goes to \\(\\exp(0) = 1\\). By differentiating the CDF we obtain the density (with respect to Lebesgue measure), which is \\[\n\\frac{1}{\\beta} \\exp \\left( - \\frac{x - \\mu}{\\beta} -\\exp \\left( - \\frac{x - \\mu}{\\beta} \\right) \\right),\n\\] so the distribution is a continuous one.\nSimulating from this distribution is straight-forward using the inverse CDF trick: simulate \\(U \\sim \\operatorname{Unif}(0,1)\\) and let \\[G = F^{-1}(U) = -\\beta\\log( -\\log (U))  + \\mu,\\] then \\(G \\sim \\operatorname{Gumbel}(\\mu, \\beta)\\). Let’s draw some samples from the standard Gumbel distribution, i.e. where \\(\\mu = 0\\) and \\(\\beta = 1\\).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sim_gumbel(n, mu, beta):\n    return - beta * np.log(-np.log(np.random.uniform(size=n))) + mu\n\ndef density_gumbel(x,mu,beta):\n    return (1/beta) * np.exp(-(x-mu)/beta-np.exp(-(x-mu)/beta))\n\nsims = sim_gumbel(1000, 0, 1)\nxmin, xmax = min(sims), max(sims)\nplt.hist(sims, bins=30, edgecolor='black', density=True)\nplt.plot(np.linspace(xmin, xmax, 1000), density_gumbel(np.linspace(xmin, xmax, 1000), 0, 1), 'r')\nplt.title(f\"{len(sims)} Gumbel(0,1) draws with mean {np.mean(sims):.2f} and standard deviation {np.std(sims):.2f}\")\nplt.axvline(np.mean(sims), color='grey', lw=2, linestyle=\"--\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom these simulations and the right skew visible in the density, we can guess that the mean of the standard Gumbel is not \\(0\\). Indeed, if \\(G \\sim \\operatorname{Gumbel}(0,1)\\), then 1 \\[\n    \\mathbf E G = \\int_{0}^1 F^{-1}(u) \\mathrm d u = \\int_{0}^1 -\\log( - \\log u) \\mathrm d u = \\gamma \\approx 0.5772,\n\\] where \\(\\gamma\\) is the Euler-Mascheroni constant.\n1 see wikipedia, I have not been able to find a concrete proof of this yetFrom the definition, we can see that the Gumbel distributions form a location-scale family, i.e. if \\(G \\sim  \\operatorname{Gumbel}(\\mu, \\beta)\\), then \\(aG + b \\sim \\operatorname{Gumbel}(a\\mu + b, a^2\\beta)\\). Similar to the normal distribution, this allows us to focus on the standard Gumbel distribution \\(\\operatorname{Gumbel}(0,1)\\).\nThe origins of the Gumbel distribution go back to the early 1930s, when Gumbel discovered the distribution as a limiting distribution of the maximum i.i.d. exponentially distributed samples. This makes the Gumbel distribution one of the three GEV (generalized extreme value) distributions, the others being the Fréchet and the reverse Weibull distribution. Actually, if \\(X \\sim \\operatorname{Gumbel}(0,1)\\), then \\(\\exp (X)\\) and \\(- \\exp(-X)\\) follow a Fréchet and a reverse Weibull distribution respectively.\n\n\n\n\n\n\nTheorem (limit theorem for the Gumbel distribution)\n\n\n\nFor \\(i \\in \\mathbf N\\) let \\(X_i \\stackrel{\\text{i.i.d.}}{\\sim} \\operatorname{Exp}(1)\\) and let \\(Y_n = \\max \\{X_1, \\dots, X_n\\}\\) be the maximum value in the first \\(n\\) samples. Then, as \\(n \\to \\infty\\), \\[\n    Y_{n} - \\log n \\stackrel{\\mathcal D}{\\longrightarrow} \\operatorname{Gumbel}(0, 1).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe CDF of \\(Y_n - \\log n\\) is, for \\(y \\geq - \\log n\\), \\[\n\\begin{align*}\n    \\mathbf P \\left( Y_{n} - \\log n \\leq y \\right) &=  \\mathbf P \\left( Y_{n} \\leq y + \\log n \\right) = (1 - \\exp(-(y + \\log n)))^{n} \\\\\n    &= \\left(1 - \\frac{\\exp(-y)}{n}\\right)^{n}\\to \\exp(-\\exp(-y)) = F(y).\n\\end{align*}\n\\]\n\n\n\nAgain, let’s verify by simulation that this is true, comparing to the density of the standard Gumbel distribution.\n\n\nCode\ndef sim_exponential(n, lambd):\n    return -1/lambd * np.log(np.random.uniform(size=n))\n\n\nfig, axs = plt.subplots(3,2)\nfig.tight_layout()\nns = [2, 4, 10, 20, 50, 100]\nm = 10000\n\nfor ax, n in zip(axs.flatten(), ns):\n    X = sim_exponential((m, n), 1)\n    Y = np.max(X, axis = 1)\n    Z = Y - np.log(n)\n    min_z, max_z = min(Z), max(Z)\n    ax.hist(Z, bins=100, density=True)\n    ax.plot(np.linspace(min_z, max_z, 1000), density_gumbel(np.linspace(min_z, max_z, 1000), 0, 1), 'r')\n    ax.set_title(f\"distribution of $Y_{{ {n} }} - \\\\log {n}$\")\n    # mode of gumbel distribution is at mu\n    ax.set_ylim(0, 1.2* density_gumbel(0,0,1))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see, already for \\(n=10\\) there is good fit between the distribution of \\(Y_n - \\log n\\) and \\(\\operatorname{Gumbel}(0, 1)\\).\nIn the limit theorem, we had to subtract \\(\\log n\\) from the maximum. Intuitively, this is necessary to ensure that the maximum does not diverge to \\(\\infty\\), so we obtain an actual distribution in the limit. This is similar to subtracting the mean in the central limit theorem (CLT).\nSimilar to the CLT, the limit theorem holds for a much larger class of distributions, not just exponential distributions and similar to the CLT we have to stabilize the maxima to obtain a valid limit. The next theorem makes this precise.\n\n\n\n\n\n\nTheorem (limit theorem for extrema) (Johnson, Kotz, and Balakrishnan 1995)\n\n\n\nLet \\(X_i, i \\in \\mathbf N\\) be a sequence of i.i.d. random variables, let \\(Y_n = \\max_{i = 1, \\dots, n} X_i\\) be the running maximum and consider \\[\n    Z_{n} = Y_{n} - b_{n},\n\\] for a sequence of real numbers \\(b_n\\), such that for every \\(k\\) \\(b_{kn} - b_n\\) converges as \\(n\\to \\infty\\) for every \\(k\\).2\nIf \\(Z_n\\) converges in distribution to a distribution with an injective CDF3, then the limiting distribution is a Gumbel distribution.\n\n\n3 again, could not get rid of this either2 I could not get rid of this technical assumption, see the sidenotes in the proof.\n\n\n\n\n\nProof\n\n\n\n\n\nI basically follow (Johnson, Kotz, and Balakrishnan 1995) in this proof. Let \\(G\\) be the CDF of the limiting distribution and \\(Z \\sim G\\) and denote general CDFs by \\(F\\). We have to show that \\[\n    G(x) = \\exp \\left( - \\exp \\left( -\\frac{x - \\mu}{\\beta} \\right) \\right)\n\\] for some \\(\\mu \\in \\mathbf R\\) and \\(\\beta \\in \\mathbf R_{&gt; 0}\\). Let \\(k\\in\\mathbf N\\) and partition the random variables into blocks of size \\(n\\), and consider the block-wise maximum, i.e.\n\\[\n    \\begin{align*}\n        Y^{j}_{n} = \\max_{i = 1, \\dots, n} X_{(j - 1)k + i} && j = 1, \\dots, k.\n    \\end{align*}\n\\] Let \\(Z^j_n = Y^j_n - b_n\\) and let \\(n \\to \\infty\\). Then \\(Z^j_n \\stackrel{\\mathcal D}{\\longrightarrow} Z\\) for \\(j = 1, \\dots, k\\).\nNow \\(Z_{kn} = \\max_{j = 1, \\dots, k} Z^j_n + b_{n} - b_{kn}\\) also converges to \\(Z\\) in distribution. By the properties of the CDF and the i.i.d. assumption, we have\n\\[\n    F_{Z_{kn}} (z) = \\mathbf P \\left( \\max_{j = 1, \\dots k} Z^{j}_n \\leq z - b_{n} + b_{kn} \\right)  = (F_{Z^{1}_n}(z - b_{n} + b_{kn}))^{k}.\n\\] As \\(n\\) goes to \\(\\infty\\), the left hand side converges to \\(G(z)\\), and so the right-hand side does as well. Assuming \\(-b_n + b_{kn} \\to c_{k}\\) as \\(n\\to \\infty\\), the right-hand side converges to \\(G(z - c_k)^k\\) as well. 4 Thus \\[\n    G(z) = G(z - c_{k})^{k},\n\\] or, equivalently, \\[\n    G(z + c_{k}) = G(z)^{k}.\n\\] This implies \\[\n    G(z + c_{k} + c_{l}) = (G(z)^{k})^{l} = G(z)^{kl} = G(z + c_{kl}),\n\\] so \\(c_k + c_l = c_{kl}\\), if \\(G\\) is injective 5. Thus \\(c_k = \\xi\\log k\\) for some \\(\\xi \\in \\mathbf R\\).\nTaking logs twice, we obtain \\[\n    \\log k + \\log (- \\log G(z)) = \\log - \\log G(z + \\xi \\log k),\n\\] and so \\(z \\mapsto \\log (- \\log G(z))\\) is an affine function, which was just what we had to show.\n\n\n\n5 again, (Johnson, Kotz, and Balakrishnan 1995) skip over this4 (Johnson, Kotz, and Balakrishnan 1995) as well as wikipedia directly use \\(-b_n + b_{kn} = b_k\\), but I haven’t seen a direct proof yet. (Haan and Ferreira 2006) goes a different, more precise route.Finally, to check whether this works, let us perform some simulations for the maximum of \\(n\\) standard normal draws.\n\n\nCode\nfrom scipy.stats.distributions import norm\n\nfig, axs = plt.subplots(3,2, figsize=(15,5))\nfig.tight_layout()\nns = [2, 4, 10, 20, 50, 100]\nm = 100000\n\nfor ax, n in zip(axs.flatten(), ns):\n    X = np.random.normal(0, 1, (m, n))\n    Y = np.max(X, axis = 1)\n    min_z, max_z = min(Y), max(Y)\n    ax.hist(Y, bins=100, density=True)\n    mu = norm.ppf(1-1/n)\n    beta = norm.ppf(1 - 1 / n / np.exp(1)) - mu\n    ax.plot(np.linspace(min_z, max_z, 1000), density_gumbel(np.linspace(min_z, max_z, 1000), mu, beta), 'r')\n    ax.set_title(f\"max of {n} standard normal draws\")\n    # mode of gumbel distribution is at mu\n    ax.set_ylim(0, 1.2* density_gumbel(mu,mu,beta))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHaan, L. de, and Ana Ferreira. 2006. Extreme Value Theory: An Introduction. Springer Series in Operations Research. New York ; London: Springer.\n\n\nJohnson, Norman L., Samuel Kotz, and Narayanaswamy Balakrishnan. 1995. Continuous Univariate Distributions. Vol. II. John Wiley & Sons, Ltd."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Optimal Gaussian Importance Sampling for Bayesian Inverse Problems on August 12 2024 @ Bernoulli-ims 11th World Congress in Probability and Statistics, Bochum, Germany\nExploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling on February 18 2024 @ Kolloquium of SFB 1294: Data Assimilation, Potsdam, Germany, joint work with Thomas Hotz"
  },
  {
    "objectID": "talks.html#section",
    "href": "talks.html#section",
    "title": "Talks",
    "section": "",
    "text": "Optimal Gaussian Importance Sampling for Bayesian Inverse Problems on August 12 2024 @ Bernoulli-ims 11th World Congress in Probability and Statistics, Bochum, Germany\nExploiting Independence for Optimal Gaussian Importance Sampling - with application to spatio-temporal epidemiological modelling on February 18 2024 @ Kolloquium of SFB 1294: Data Assimilation, Potsdam, Germany, joint work with Thomas Hotz"
  },
  {
    "objectID": "talks.html#section-1",
    "href": "talks.html#section-1",
    "title": "Talks",
    "section": "2023",
    "text": "2023\n\nExploiting independence in Gaussian importance sampling for Bayesian inverse problems on December 15 2023 @ CMStatistics, Berlin, Germany\nPartially Gaussian State Space Models for Epidemiological Modelling on September 14 2023 @ Kolloquium KIT, Karlsruhe, Germany, joint work with Thomas Hotz\nImproving short term forecasts of COVID-19 incidence with subnational epidemic indicators on September 14 2023 @ CEN 2023, Basel, Switzerland, joint work with Thomas Hotz\nRegional estimates of epidemic growth factors with application to COVID-19 on March 08 2023 @ GPSD 2023, Essen, Germany, joint work with Jan Pablo Burgard, Tyll Krüger, Thomas Hotz\nModelling and Control of the COVID-19 Epidemic on January 25 2023 @ Wissenschaftsforum TU Ilmenau, Ilmenau, Germany, joint work with Sara Grundel, Thomas Hotz, Tobias Ritschel, Philipp Sauerteig, Karl Worthmann"
  },
  {
    "objectID": "talks.html#section-2",
    "href": "talks.html#section-2",
    "title": "Talks",
    "section": "2022",
    "text": "2022\n\nRegional estimates of reproduction numbers with application to COVID-19 on June 30 2022 @ IMS 2022, London, UK, joint work with Jan Pablo Burgard, Tyll Krüger, Thomas Hotz\nRegional estimates of reproduction numbers with application to COVID-19 on March 31 2022 @ DAGStat 2022, Hamburg, Germany, joint work with Jan Pablo Burgard, Tyll Krüger, Thomas Hotz"
  },
  {
    "objectID": "talks.html#section-3",
    "href": "talks.html#section-3",
    "title": "Talks",
    "section": "2019",
    "text": "2019\n\nNon-asymptotic confidence sets for Frechet means on July 22 2019 @ EMS 2019, Palermo, Italy, joint work with Thomas Hotz"
  },
  {
    "objectID": "talks.html#section-4",
    "href": "talks.html#section-4",
    "title": "Talks",
    "section": "2018",
    "text": "2018\n\nBenfords Gesetz on October 26 2018 @ Kinderuni TU Ilmenau, Ilmenau, Germany, joint work with Thomas Hotz\nKalman filter on Lie groups on June 26 2018 @ S4G 2018, Prague, Czech Republic, joint work with Thomas Hotz"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Early data validation saves you trouble down the line\n\n\n\n\n\n\ndata\n\n\ndata science\n\n\n\n\n\n\n\n\n\nMay 29, 2025\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nThe Gumbel distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nAsymptotics of estimators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nFisherian reduction for the t-Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nStefan Heyder\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "This is me, Stefan Heyder. I am currently working as a Senior Data Scientist for singularIT and am finishing my PhD degree at Technische Universität Ilmenau.\nI will sporadically use this blog to process my thoughts on computational statistics, data science and (new!) project management. This might include procrastination on my PhD thesis."
  }
]