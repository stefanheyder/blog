[
  {
    "objectID": "impressum.html",
    "href": "impressum.html",
    "title": "Impressum",
    "section": "",
    "text": "Stefan Heyder\nWiesenweg 21, 98693 Ilmenau\nstefan.heyder@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "The Gumbel distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 9, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nAsymptotics of estimators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nStefan Heyder\n\n\n\n\n\n\n\n\n\n\n\n\nFisherian reduction for the t-Test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2022\n\n\nStefan Heyder\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html",
    "href": "posts/2024-07-07 asymptotics/index.html",
    "title": "Asymptotics of estimators",
    "section": "",
    "text": "In my PhD thesis, I compare two methods to perform optimal importance sampling: the Cross-Entropy method (CE) and Efficient Importance Sampling (EIS). There are several criteria that you may want to consider for this comparison, but without going into too much detail , in this post, I’ll focus on only one aspect: asymptotics, in particular asymptotic relative efficiencies.If you’re interested, check out the draft of my thesis on Github.\nIn the context of my thesis, this crops up as both methods are simulation-based. Usually, we have to resort to simulation techniques when analytical computation is too difficult, which is exactly the case here! Both methods actually want to solve an optimization problem in the background, solving for an optimal parameter \\(\\psi\\). Unfortunately, the optimization problems involve quantities that we have no (analytical) access to. We can, however, set up a simulation routine that provides an estimate \\(\\hat\\psi\\) of this optimal parameter.\nBut using simulations means that the output of both methods is now random, i.e. we can expect to get different results \\(\\hat\\psi\\) when we repeatedly apply the methods using different RNG seeds. This can be problematic: if there is too much variation the actual \\(\\hat\\psi\\) we obtain could be far away from the optimal \\(\\psi\\) and the performance of the methods suffers.\nThis is where asymptotics come into play. When statisticians talk about asymptotics for estimators, here \\(\\hat\\psi\\), they are usually concerned with two things: consistency and asymptotic normality. Consistency is a critical property of an estimator."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html#asymptotic-normality",
    "href": "posts/2024-07-07 asymptotics/index.html#asymptotic-normality",
    "title": "Asymptotics of estimators",
    "section": "Asymptotic normality",
    "text": "Asymptotic normality\n\n\n\n\n\n\nDefinition (central limit theorem)\n\n\n\nAgain, let \\((\\hat\\psi_{N})_{N \\in \\mathbf N}\\) be a sequence of estimator of \\(\\psi\\). If \\[\n\\sqrt {N} \\left( \\hat \\psi_N - \\psi \\right) \\stackrel{\\mathcal D}{\\to} \\mathcal N(0, \\Sigma),\n\\] we say that \\(\\hat\\psi_{N}\\) fulfills a central limit theorem with asymptotic covariance matrix \\(\\Sigma\\).\n\n\nIf we have a consistent estimator, notice that \\(\\hat\\psi_N - \\psi\\) goes to \\(0\\) as \\(N\\) goes to \\(\\infty\\) (in probability or almost surely, depending on the type of consistency). Multiplying by \\(\\sqrt{N}\\) “blows up” this error, essentially zooming in to what happens around the true value \\(\\psi\\). Note that any estimator that fulfills a central limit theorem is weakly consistent.\nWhy should we assume a normal distribution as the limiting distribution? At first this choice may seem surprising, maybe even restrictive. But it turns out that for large classes of estimators, e.g. M- and Z-estimators, we can proof such a central limit theorem.\nHow does having a central limit theorem help us? If we know \\(\\Sigma\\), we can use it get a heuristic on how large we should choose \\(N\\). Using the approximation \\[\n\\hat\\psi_{N} -\\psi \\approx \\mathcal N\\left(0, \\frac{\\Sigma}{N}\\right)\n\\] we can choose \\(N\\) such that, e.g., for a given error \\(\\varepsilon &gt; 0\\), the probability that \\(\\lVert \\hat \\psi_N - \\psi \\rVert &lt; \\varepsilon\\) becomes, say, at least \\(80\\%\\).\nAdditionally, if we have two estimators \\(\\hat \\psi^1\\) and \\(\\hat\\psi^{2}\\), both of which fulfill a central limit theorem with asymptotic covariance matrices \\(\\Sigma_1\\) and \\(\\Sigma_{2}\\), we can compare \\(\\Sigma_1\\) and \\(\\Sigma_2\\). The estimator with the “smaller” asymptotic covariance matrix is the one we should prefer.\nAs we are dealing with matrices it is not necessarily clear what smaller means, we could be interested in, e.g.\n\n\\(\\Sigma_1 \\succ \\Sigma_{2}\\), i.e. \\(\\Sigma_1 - \\Sigma_2\\) is postive definite, or\n\\(\\operatorname{trace} \\left( \\Sigma_{1} \\right) &gt; \\operatorname{trace} \\left( \\Sigma_{2} \\right)\\), i.e. the asymptotic mean squared error (MSE) is smaller for \\(\\psi_2\\).\n\nUsually the asymptotic covariance matrix \\(\\Sigma\\) is not known, usually because it depends on either the true parameter \\(\\psi\\). For a toy example, consider the normal distribution \\(\\mathcal N(\\mu, \\sigma^2)\\) where the mean \\(\\mu \\in \\mathbf R\\) and variance is \\(\\sigma^2 \\in \\mathbf R_{&gt; 0}\\). Then the sample mean \\(\\bar X_{N}\\) for samples \\(X_{1}, \\dots, X_{N} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal N(\\mu, \\sigma^{2})\\) as an estimator of \\(\\mu\\) fulfills a central limit theorem: \\[\n\\sqrt{N} (\\bar X_{N} - \\mu) \\stackrel{\\mathcal D}\\to \\mathcal N(0, \\sigma^{2}).\n\\] Here \\(\\sigma^2\\) may be unknown. In practice we can estimate it consistently by the sample variance \\(\\hat \\sigma_N^{2}\\), and so \\[\n\\frac{\\sqrt{N}}{\\sqrt{\\hat \\sigma_{N}^2}} \\left( \\bar X_{N} - \\mu \\right) \\stackrel{\\mathcal D}\\to \\mathcal N(0,1),\n\\] by Slutsky’s lemma. Thus, for large \\(N\\), we can the true asymptotic variance \\(\\sigma^2\\) by its consistent estimate \\(\\hat\\sigma_N^2\\), and proceed as above."
  },
  {
    "objectID": "posts/2024-07-07 asymptotics/index.html#whats-next",
    "href": "posts/2024-07-07 asymptotics/index.html#whats-next",
    "title": "Asymptotics of estimators",
    "section": "What’s next?",
    "text": "What’s next?\nIn this post, I’ve tried to convey the usefulness of asymptotics, especially when it comes to comparing two estimators \\(\\psi\\). However, I have not given you any guidance on when an estimator is consistent and when it fulfills a central limit theorem. It turns out, that for many types of estimators we can obtain, under some assumptions, central limit theorems. These settings I will discuss in a following post."
  },
  {
    "objectID": "posts/2022-02-08 fisherian_reduction/index.html",
    "href": "posts/2022-02-08 fisherian_reduction/index.html",
    "title": "Fisherian reduction for the t-Test",
    "section": "",
    "text": "In the second chapter of (Cox 2006) the authors talks about a Fisherian reduction which I think of as a framework of doing inference given a sufficient statistic \\(S\\). An interesting point here is that one can use the conditional distribution of the data, \\(X_1, \\dots, X_n\\) say, on \\(S\\) to evaluate the fit of the model.\nIn this post I want to explore this concept in the setting of a standard \\(t\\)-Test, i.e. we have \\(X_i \\overset{\\text{i.i.d}}{\\sim} \\mathcal N(\\mu, \\sigma^2)\\). The parameter of interest is of course \\(\\mu\\) and \\(S = (\\bar X_n, \\hat\\sigma^2_{n})\\) is a sufficient statistic, with \\(\\hat\\sigma^2_n\\) the empirical variance.\nTo apply the Fisherian reduction we thus need to find the conditional distribution of \\(X = \\left(X_1, \\dots, X_n\\right)\\) on \\(\\bar X_n\\), i.e.\\(X | \\bar X_n\\). For this, let \\(A = \\left(A_1, B\\right)  \\in \\mathbf R^{n\\times n}\\) be an orthogonal matrix whose first column is \\[A_1 = \\left(\\frac 1 {\\sqrt{n}}, \\dots , \\frac 1 {\\sqrt{n}} \\right).\\]\nThen \\(Y = A^TX \\sim \\mathcal N \\left(\\mu A^T\\mu \\mathbf 1, \\sigma^2 I_{n}\\right)\\) and \\(Y = \\left(\\sqrt{n} \\bar X_n, Z\\right)\\) where \\(Z \\sim \\mathcal N \\left(\\sigma^2 I_{n - 1}\\right)\\), \\(Z\\) and \\(\\bar X_n\\) being independent.\nTransforming back we obtain the conditional distribution we sought: \\[ X | \\bar X_n \\sim AY | \\bar X_n \\mathbf 1 \\sim \\bar X_n + B Z\\]\nThe catch here is that \\(B\\) is a \\(n\\times (n - 1)\\) dimensional matrix, so \\(X | \\bar X_n\\) has a normal distribution of dimension \\(n-1\\), e.g. the variance-covariance matrix is rank-deficient.\nLet’s verify this through simulation.\n\n\nCode\nmu &lt;- 10\nsigma &lt;- 1\nn &lt;- 2\nm &lt;- 1000\nx &lt;- matrix(rnorm(n * m, mu, sigma), nrow= n)\n\ny &lt;- t(t(x) - colMeans(x))\n\ntransformed &lt;- c(matrix(c(1/sqrt(2), -1/sqrt(2)), nrow = 1) %*% y)\n\nplot(y[1,], y[2,])\n\nhist(transformed)\nprint(mean(transformed))\nprint(sd(transformed))\n\n\n\n\n\n\n\n\n\n[1] 0.007553103\n[1] 1.05018\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCox, D. R. 2006. Principles of Statistical Inference. Cambridge ; New York: Cambridge University Press."
  },
  {
    "objectID": "posts/2024-07-17 gumbel_distribution/index.html",
    "href": "posts/2024-07-17 gumbel_distribution/index.html",
    "title": "The Gumbel distribution",
    "section": "",
    "text": "I recently learned about the Gumbel softmax trick, which seemingly allows smooth sampling from a discrete distribution. In writing this post, I want to learn more about the Gumbel distribution that appears in this trick, details on the trick may follow in a separate post.\nThe main component in the softmax trick is the following distribution, due to Gumbel (aptly named after him).\n\n\n\n\n\n\nDefinition (Gumbel distribution)\n\n\n\nFor \\(\\mu \\in \\mathbf R\\) and \\(\\beta \\in \\mathbf R_{&gt; 0}\\) the \\(\\operatorname{Gumbel}(\\mu, \\beta)\\) distribution has the CDF \\[\n\\begin{align*}\n    F: (-\\infty, \\infty) \\to (0, 1) && x \\mapsto \\exp \\left( -\\exp \\left( - \\frac{x - \\mu}{\\beta} \\right)  \\right).\n\\end{align*}\n\\]\n\n\nLet us quickly ensure that this defines a distribution: this CDF is strictly monotonically increasing and for \\(x \\to -\\infty\\) it goes to \\(0\\), while for \\(x \\to \\infty\\) it goes to \\(\\exp(0) = 1\\). By differentiating the CDF we obtain the density (with respect to Lebesgue measure), which is \\[\n\\frac{1}{\\beta} \\exp \\left( - \\frac{x - \\mu}{\\beta} -\\exp \\left( - \\frac{x - \\mu}{\\beta} \\right) \\right),\n\\] so the distribution is a continuous one.\nSimulating from this distribution is straight-forward using the inverse CDF trick: simulate \\(U \\sim \\operatorname{Unif}(0,1)\\) and let \\[G = F^{-1}(U) = -\\beta\\log( -\\log (U))  + \\mu,\\] then \\(G \\sim \\operatorname{Gumbel}(\\mu, \\beta)\\). Let’s draw some samples from the standard Gumbel distribution, i.e. where \\(\\mu = 0\\) and \\(\\beta = 1\\).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef sim_gumbel(n, mu, beta):\n    return - beta * np.log(-np.log(np.random.uniform(size=n))) + mu\n\ndef density_gumbel(x,mu,beta):\n    return (1/beta) * np.exp(-(x-mu)/beta-np.exp(-(x-mu)/beta))\n\nsims = sim_gumbel(1000, 0, 1)\nxmin, xmax = min(sims), max(sims)\nplt.hist(sims, bins=30, edgecolor='black', density=True)\nplt.plot(np.linspace(xmin, xmax, 1000), density_gumbel(np.linspace(xmin, xmax, 1000), 0, 1), 'r')\nplt.title(f\"{len(sims)} Gumbel(0,1) draws with mean {np.mean(sims):.2f} and standard deviation {np.std(sims):.2f}\")\nplt.axvline(np.mean(sims), color='grey', lw=2, linestyle=\"--\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFrom these simulations and the right skew visible in the density, we can guess that the mean of the standard Gumbel is not \\(0\\). Indeed, if \\(G \\sim \\operatorname{Gumbel}(0,1)\\), then  \\[\n    \\mathbf E G = \\int_{0}^1 F^{-1}(u) \\mathrm d u = \\int_{0}^1 -\\log( - \\log u) \\mathrm d u = \\gamma \\approx 0.5772,\n\\] where \\(\\gamma\\) is the Euler-Mascheroni constant.see wikipedia, I have not been able to find a concrete proof of this yet\nFrom the definition, we can see that the Gumbel distributions form a location-scale family, i.e. if \\(G \\sim  \\operatorname{Gumbel}(\\mu, \\beta)\\), then \\(aG + b \\sim \\operatorname{Gumbel}(a\\mu + b, a^2\\beta)\\). Similar to the normal distribution, this allows us to focus on the standard Gumbel distribution \\(\\operatorname{Gumbel}(0,1)\\).\nThe origins of the Gumbel distribution go back to the early 1930s, when Gumbel discovered the distribution as a limiting distribution of the maximum i.i.d. exponentially distributed samples. This makes the Gumbel distribution one of the three GEV (generalized extreme value) distributions, the others being the Fréchet and the reverse Weibull distribution. Actually, if \\(X \\sim \\operatorname{Gumbel}(0,1)\\), then \\(\\exp (X)\\) and \\(- \\exp(-X)\\) follow a Fréchet and a reverse Weibull distribution respectively.\n\n\n\n\n\n\nTheorem (limit theorem for the Gumbel distribution)\n\n\n\nFor \\(i \\in \\mathbf N\\) let \\(X_i \\stackrel{\\text{i.i.d.}}{\\sim} \\operatorname{Exp}(1)\\) and let \\(Y_n = \\max \\{X_1, \\dots, X_n\\}\\) be the maximum value in the first \\(n\\) samples. Then, as \\(n \\to \\infty\\), \\[\n    Y_{n} - \\log n \\stackrel{\\mathcal D}{\\longrightarrow} \\operatorname{Gumbel}(0, 1).\n\\]\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nThe CDF of \\(Y_n - \\log n\\) is, for \\(y \\geq - \\log n\\), \\[\n\\begin{align*}\n    \\mathbf P \\left( Y_{n} - \\log n \\leq y \\right) &=  \\mathbf P \\left( Y_{n} \\leq y + \\log n \\right) = (1 - \\exp(-(y + \\log n)))^{n} \\\\\n    &= \\left(1 - \\frac{\\exp(-y)}{n}\\right)^{n}\\to \\exp(-\\exp(-y)) = F(y).\n\\end{align*}\n\\]\n\n\n\nAgain, let’s verify by simulation that this is true, comparing to the density of the standard Gumbel distribution.\n\n\nCode\ndef sim_exponential(n, lambd):\n    return -1/lambd * np.log(np.random.uniform(size=n))\n\n\nfig, axs = plt.subplots(3,2)\nfig.tight_layout()\nns = [2, 4, 10, 20, 50, 100]\nm = 10000\n\nfor ax, n in zip(axs.flatten(), ns):\n    X = sim_exponential((m, n), 1)\n    Y = np.max(X, axis = 1)\n    Z = Y - np.log(n)\n    min_z, max_z = min(Z), max(Z)\n    ax.hist(Z, bins=100, density=True)\n    ax.plot(np.linspace(min_z, max_z, 1000), density_gumbel(np.linspace(min_z, max_z, 1000), 0, 1), 'r')\n    ax.set_title(f\"distribution of $Y_{{ {n} }} - \\\\log {n}$\")\n    # mode of gumbel distribution is at mu\n    ax.set_ylim(0, 1.2* density_gumbel(0,0,1))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs you can see, already for \\(n=10\\) there is good fit between the distribution of \\(Y_n - \\log n\\) and \\(\\operatorname{Gumbel}(0, 1)\\).\nIn the limit theorem, we had to subtract \\(\\log n\\) from the maximum. Intuitively, this is necessary to ensure that the maximum does not diverge to \\(\\infty\\), so we obtain an actual distribution in the limit. This is similar to subtracting the mean in the central limit theorem (CLT).\nSimilar to the CLT, the limit theorem holds for a much larger class of distributions, not just exponential distributions and similar to the CLT we have to stabilize the maxima to obtain a valid limit. The next theorem makes this precise.\n\n\n\n\n\n\nTheorem (limit theorem for extrema) (Johnson, Kotz, and Balakrishnan 1995)\n\n\n\nLet \\(X_i, i \\in \\mathbf N\\) be a sequence of i.i.d. random variables, let \\(Y_n = \\max_{i = 1, \\dots, n} X_i\\) be the running maximum and consider \\[\n    Z_{n} = Y_{n} - b_{n},\n\\] for a sequence of real numbers \\(b_n\\), such that for every \\(k\\) \\(b_{kn} - b_n\\) converges as \\(n\\to \\infty\\) for every \\(k\\).\nIf \\(Z_n\\) converges in distribution to a distribution with an injective CDF, then the limiting distribution is a Gumbel distribution.\n\nI could not get rid of this technical assumption, see the sidenotes in the proof.again, could not get rid of this either\n\n\n\n\n\n\nProof\n\n\n\n\n\nI basically follow (Johnson1995Continous?) in this proof. Let \\(G\\) be the CDF of the limiting distribution and \\(Z \\sim G\\) and denote general CDFs by \\(F\\). We have to show that \\[\n    G(x) = \\exp \\left( - \\exp \\left( -\\frac{x - \\mu}{\\beta} \\right) \\right)\n\\] for some \\(\\mu \\in \\mathbf R\\) and \\(\\beta \\in \\mathbf R_{&gt; 0}\\). Let \\(k\\in\\mathbf N\\) and partition the random variables into blocks of size \\(n\\), and consider the block-wise maximum, i.e.\n\\[\n    \\begin{align*}\n        Y^{j}_{n} = \\max_{i = 1, \\dots, n} X_{(j - 1)k + i} && j = 1, \\dots, k.\n    \\end{align*}\n\\] Let \\(Z^j_n = Y^j_n - b_n\\) and let \\(n \\to \\infty\\). Then \\(Z^j_n \\stackrel{\\mathcal D}{\\longrightarrow} Z\\) for \\(j = 1, \\dots, k\\).\nNow \\(Z_{kn} = \\max_{j = 1, \\dots, k} Z^j_n + b_{n} - b_{kn}\\) also converges to \\(Z\\) in distribution. By the properties of the CDF and the i.i.d. assumption, we have\n\\[\n    F_{Z_{kn}} (z) = \\mathbf P \\left( \\max_{j = 1, \\dots k} Z^{j}_n \\leq z - b_{n} + b_{kn} \\right)  = (F_{Z^{1}_n}(z - b_{n} + b_{kn}))^{k}.\n\\] As \\(n\\) goes to \\(\\infty\\), the left hand side converges to \\(G(z)\\), and so the right-hand side does as well. Assuming \\(-b_n + b_{kn} \\to c_{k}\\) as \\(n\\to \\infty\\), the right-hand side converges to \\(G(z - c_k)^k\\) as well.  Thus \\[\n    G(z) = G(z - c_{k})^{k},\n\\] or, equivalently, \\[\n    G(z + c_{k}) = G(z)^{k}.\n\\] This implies \\[\n    G(z + c_{k} + c_{l}) = (G(z)^{k})^{l} = G(z)^{kl} = G(z + c_{kl}),\n\\] so \\(c_k + c_l = c_{kl}\\), if \\(G\\) is injective [again, (Johnson1995Continous?) skip over this]. Thus \\(c_k = \\xi\\log k\\) for some \\(\\xi \\in \\mathbf R\\).\nTaking logs twice, we obtain \\[\n    \\log k + \\log (- \\log G(z)) = \\log - \\log G(z + \\xi \\log k),\n\\] and so \\(z \\mapsto \\log (- \\log G(z))\\) is an affine function, which was just what we had to show.\n\n\n\n(Johson1995Continuous?) as well as wikipedia directly use \\(-b_n + b_{kn} = b_k\\), but I haven’t seen a direct proof yet. (Haan and Ferreira 2006) goes a different, more precise route.Finally, to check whether this works, let us perform some simulations for the maximum of \\(n\\) standard normal draws.\n\n\nCode\nfrom scipy.stats.distributions import norm\n\nfig, axs = plt.subplots(3,2, figsize=(15,5))\nfig.tight_layout()\nns = [2, 4, 10, 20, 50, 100]\nm = 100000\n\nfor ax, n in zip(axs.flatten(), ns):\n    X = np.random.normal(0, 1, (m, n))\n    Y = np.max(X, axis = 1)\n    min_z, max_z = min(Y), max(Y)\n    ax.hist(Y, bins=100, density=True)\n    mu = norm.ppf(1-1/n)\n    beta = norm.ppf(1 - 1 / n / np.exp(1)) - mu\n    ax.plot(np.linspace(min_z, max_z, 1000), density_gumbel(np.linspace(min_z, max_z, 1000), mu, beta), 'r')\n    ax.set_title(f\"max of {n} standard normal draws\")\n    # mode of gumbel distribution is at mu\n    ax.set_ylim(0, 1.2* density_gumbel(mu,mu,beta))\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nHaan, L. de, and Ana Ferreira. 2006. Extreme Value Theory: An Introduction. Springer Series in Operations Research. New York ; London: Springer.\n\n\nJohnson, Norman L., Samuel Kotz, and Narayanaswamy Balakrishnan. 1995. Continuous Univariate Distributions. Vol. II. John Wiley & Sons, Ltd."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "This is me, Stefan Heyder. I am currently finishing my PhD degree at Technische Universität Ilmenau.\nI will sporadically use this blog to process my thoughts on computational statistics. This might include procrastination on my PhD thesis."
  }
]